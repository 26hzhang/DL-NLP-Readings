# Machine Translation and Language Transfer, Generation, Summarization and Dialogue

## Machine Translation
- [2014 SSST] **On the properties of neural machine Translation Encoder-Decoder Approaches**, [[paper]](https://www.aclweb.org/anthology/W14-4012.pdf), [[bibtex]](https://www.aclweb.org/anthology/W14-4012.bib).
- [2015 ICLR] **Neural Machine Translation by Jointly Learning to Align and Translate**, [[paper]](https://arxiv.org/pdf/1409.0473.pdf), [[bibtex]](/Bibtex/Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.bib), sources: [[lisa-groundhog/GroundHog]](https://github.com/lisa-groundhog/GroundHog/tree/master/experiments/nmt), [[tensorflow/nmt]](https://github.com/tensorflow/nmt).
- [2015 EMNLP] **Effective Approaches to Attention-based Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/D15-1166.pdf), [[bibtex]](https://www.aclweb.org/anthology/D15-1166.bib), [[HarvardNLP homepage]](http://nlp.seas.harvard.edu/code/), sources: [[dillonalaird/Attention]](https://github.com/dillonalaird/Attention), [[tensorflow/nmt]](https://github.com/tensorflow/nmt).
- [2016 ACL] **Neural Machine Translation of Rare Words with Subword Units**, [[paper]](http://www.aclweb.org/anthology/P16-1162), [[bibtex]](/Bibtex/Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subword%20Units.bib), [[software]](http://anthology.aclweb.org/attachments/P/P16/P16-1162.Software.zip), sources: [[rsennrich/subword-nmt]](https://github.com/rsennrich/subword-nmt), [[soaxelbrooke/python-bpe]](https://github.com/soaxelbrooke/python-bpe).
- [2016 NeurIPS] **Professor Forcing: A New Algorithm for Training Recurrent Networks**, [[paper]](http://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf), [[bibtex]](/Bibtex/Professor%20Forcing.bib), sources: [[anirudh9119/LM_GANS]](https://github.com/anirudh9119/LM_GANS).
- [2017 ACL] **A Convolutional Encoder Model for Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/P17-1012.pdf), [[bibtex]](https://www.aclweb.org/anthology/P17-1012.bib), sources: [[facebookresearch/fairseq]](https://github.com/facebookresearch/fairseq).
- [2017 NIPS] **Attention is All You Need**, [[paper]](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf), [[bibtex]](/Bibtex/Attention%20is%20All%20You%20Need.bib), [[Chinses blog]](http://www.cnblogs.com/robert-dlut/p/8638283.html), sources: [[Kyubyong/transformer]](https://github.com/Kyubyong/transformer), [[jadore801120/attention-is-all-you-need-pytorch]](https://github.com/jadore801120/attention-is-all-you-need-pytorch), [[DongjunLee/transformer-tensorflow]](https://github.com/DongjunLee/transformer-tensorflow).
- [2017 ICML] **Convolutional Sequence to Sequence Learning**, [[paper]](https://arxiv.org/pdf/1705.03122v3.pdf), [[bibtex]](/Bibtex/Convolutional%20Sequence%20to%20Sequence%20Learning.bib), sources: [[pytorch/fairseq]](https://github.com/pytorch/fairseq).
- [2017 EMNLP] **Neural Machine Translation with Word Predictions**, [[paper]](https://www.aclweb.org/anthology/D17-1013.pdf), [[bibtex]](https://www.aclweb.org/anthology/D17-1013.bib).
- [2017 EMNLP] **Massive Exploration of Neural Machine Translation Architectures**, [[paper]](https://www.aclweb.org/anthology/D17-1151.pdf), [[bibtex]](https://www.aclweb.org/anthology/D17-1151.bib), [[homepage]](https://google.github.io/seq2seq/), sources: [[google/seq2seq]](https://github.com/google/seq2seq).
- [2017 EMNLP] **Efficient Attention using a Fixed-Size Memory Representation**, [[paper]](https://www.aclweb.org/anthology/D17-1040.pdf), [[bibtex]](https://www.aclweb.org/anthology/D17-1040.bib).
- [2018 AMTA] **Context Models for OOV Word Translation in Low-Resource Language**, [[paper]](https://arxiv.org/pdf/1801.08660.pdf), [[bibtex]](/Bibtex/Context%20Models%20for%20OOV%20Word%20Translation%20in%20Low-Resource%20Language.bib).
- [2018 ACL] **Modeling Localness for Self-Attention Networks**, [[paper]](https://www.aclweb.org/anthology/D18-1475.pdf), [[bibtex]](/Bibtex/Modeling%20Localness%20for%20Self-Attention%20Networks.bib).
- [2018 NAACL] **Self-Attention with Relative Position Representations**, [[paper]](https://www.aclweb.org/anthology/N18-2074.pdf), [[bibtex]](/Bibtex/Self-Attention%20with%20Relative%20Position%20Representations.bib).
- [2018 COLING] **Double Path Networks for Sequence to Sequence Learning**, [[paper]](https://www.aclweb.org/anthology/C18-1259.pdf), [[bibtex]](https://www.aclweb.org/anthology/C18-1259.bib).
- [2018 EMNLP] **Meta-Learning for Low-Resource Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/D18-1398.pdf), [[bibtex]](/Bibtex/Meta-Learning%20for%20Low-Resource%20Neural%20Machine%20Translation.bib).
- [2019 NAACL] **Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/N19-1120), [[bibtex]](/Bibtex/Extract%20and%20Edit%20-%20An%20Alternative%20to%20Back-Translation%20for%20Unsupervised%20Neural%20Machine%20Translation.bib), sources: [[jiaweiw/Extract-Edit-Unsupervised-NMT]](https://github.com/jiaweiw/Extract-Edit-Unsupervised-NMT).
- [2019 NAACL] **Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/N19-1209), [[bibtex]](/Bibtex/Overcoming%20Catastrophic%20Forgetting%20During%20Domain%20Adaptation%20of%20Neural%20Machine%20Translation.bib).
- [2019 ACL] **From Bilingual to Multilingual Neural Machine Translation by Incremental Training**, [[paper]](https://www.aclweb.org/anthology/P19-2033), [[bibtex]](/Bibtex/From%20Bilingual%20to%20Multilingual%20Neural%20Machine%20Translation%20by%20Incremental%20Training.bib).
- [2019 ACL] **Bridging the Gap between Training and Inference for Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/P19-1426.pdf), [[bibtex]](/Bibtex/Bridging%20the%20Gap%20between%20Training%20and%20Inference%20for%20Neural%20Machine%20Translation.bib), [[论文解释1]](https://spring-quan.github.io/2019/08/02/论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》/), [[论文解释2]](https://zhuanlan.zhihu.com/p/76227765), sources: [[ictnlp/OR-NMT]](https://github.com/ictnlp/OR-NMT).
	- Gumbel-Max Technique: [2014 NIPS] **A\* Sampling**, [[paper]](https://papers.nips.cc/paper/5449-a-sampling.pdf), [[bibtex]](/Bibtex/A-Sampling.bib), [[Gumbel-max trick]](https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/), [[Gumbel trick]](https://blog.csdn.net/a358463121/article/details/80820878), [[Gumbel-Max Trick]](https://www.ntu.edu.sg/home/lixiucheng/paper/gumbel-softmax.html), [[The Gumbel trick]](https://francisbach.com/the-gumbel-trick/), [[Gumbel Distribution]](https://blog.csdn.net/jackytintin/article/details/79364490).
- [2019 EMNLP] **Mask-Predict: Parallel Decoding of Conditional Masked Language Models**, [[paper]](https://www.aclweb.org/anthology/D19-1633.pdf), [[bibtex]](https://www.aclweb.org/anthology/D19-1633.bib), sources: [[facebookresearch/Mask-Predict]](https://github.com/facebookresearch/Mask-Predict).
- [2020 ICLR] **Neural Machine Translation with Universal Visual Representation**, [[paper]](https://openreview.net/pdf?id=Byl8hhNYPS), [[bibtex]](/Bibtex/Neural%20Machine%20Translation%20with%20Universal%20Visual%20Representation.bib), sources: [[cooelf/UVR-NMT]](https://github.com/cooelf/UVR-NMT).
- [2020 ICLR] **Incorporating BERT into Neural Machine Translation**, [[paper]](https://openreview.net/pdf?id=Hyl7ygStwB), [[bibtex]](/Bibtex/Incorporating%20BERT%20into%20Neural%20Machine%20Translation.bib), sources: [[bert-nmt/bert-nmt]](https://github.com/bert-nmt/bert-nmt).
- [2020 ArXiv] **Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation**, [[paper]](https://arxiv.org/pdf/2001.08140.pdf), [[bibtex]](https://scholar.googleusercontent.com/scholar.bib?q=info:n2ekHcvBI8AJ:scholar.google.com/&output=citation&scisdr=CgU1_ws_EMa_0lNn7So:AAGBfm0AAAAAXqZi9Sq117oTyjRZ9t5QVNNuEmiQbxPu&scisig=AAGBfm0AAAAAXqZi9VoRSYrzh3HX4bWW3pQDd3QQOVo7&scisf=4&ct=citation&cd=-1&hl=en).
- [2020 ACL] **Multimodal Transformer for Multimodal Machine Translation**, [[paper]](https://www.aclweb.org/anthology/2020.acl-main.400.pdf), [[bibtex]](https://www.aclweb.org/anthology/2020.acl-main.400.bib).
- [2021 ACL Findings] **On the Language Coverage Bias for Neural Machine Translation**, [[paper]](https://aclanthology.org/2021.findings-acl.422.pdf), [[bibtex]](/Bibtex/On%20the%20Language%20Coverage%20Bias%20for%20Neural%20Machine%20Translation.bib).
- [2021 ICML] **Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation**, [[paper]](https://arxiv.org/pdf/2106.05093.pdf), [[bibtex]](/Bibtex/Order-Agnostic%20Cross%20Entropy%20for%20Non-Autoregressive%20Machine%20Translation.bib), sources: [[tencent-ailab/ICML21_OAXE]](https://github.com/tencent-ailab/ICML21_OAXE).

## Language Style Transfer
- [2018 ArXiv] **Style Transfer as Unsupervised Machine Translation**, [[paper]](https://arxiv.org/pdf/1808.07894.pdf), [[bibtex]](/Bibtex/Style%20Transfer%20as%20Unsupervised%20Machine%20Translation.bib), [[homepage]](https://zrustc.github.io).
- [2019 ICLR] **Multiple-Attribute Text Rewriting**, [[paper]](https://openreview.net/pdf?id=H1g2NhC5KQ), [[bibtex]](/Bibtex/Multiple-Attribute%20Text%20Rewriting.bib).

## Dialogue, Chatbot and Language Generation
- [2013 IEEE] **POMDP-based Statistical Spoken Dialogue Systems: a Review**, [[paper]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/young2013procieee.pdf), [[bibtex]](/Bibtex/POMDP-based%20Statistical%20Spoken%20Dialogue%20Systems.bib).
- [2014 NIPS] **Sequence to Sequence Learning with Neural Networks**, [[paper]](https://arxiv.org/pdf/1409.3215.pdf), [[bibtex]](/Bibtex/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.bib), sources: [[farizrahman4u/seq2seq]](https://github.com/farizrahman4u/seq2seq), [[ma2rten/seq2seq]](https://github.com/ma2rten/seq2seq), [[JayParks/tf-seq2seq]](https://github.com/JayParks/tf-seq2seq), [[macournoyer/neuralconvo]](https://github.com/macournoyer/neuralconvo).
- [2015 CIKM] **A Hierarchical Recurrent Encoder-Decoder for Generative Context-Aware Query Suggestion**, [[paper]](https://arxiv.org/pdf/1507.02221.pdf), [[bibtex]](/Bibtex/A%20Hierarchical%20Recurrent%20Encoder-Decoder%20for%20Generative%20Context-Aware%20Query%20Suggestion.bib), sources: [[sordonia/hred-qs]](https://github.com/sordonia/hred-qs).
- [2015 EMNLP] **Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems**, [[paper]](https://www.aclweb.org/anthology/D15-1199.pdf), [[bibtex]](https://www.aclweb.org/anthology/D15-1199.bib), sources: [[shawnwun/RNNLG]](https://github.com/shawnwun/RNNLG), [[hit-computer/SC-LSTM]](https://github.com/hit-computer/SC-LSTM).
- [2015 ArXiv] **Attention with Intention for a Neural Network Conversation Model**, [[paper]](https://arxiv.org/pdf/1510.08565.pdf), [[bibtex]](/Bibtex/Attention%20with%20Intention%20for%20a%20Neural%20Network%20Conversation%20Model.bib).
- [2015 ACL] **Neural Responding Machine for Short-Text Conversation**, [[paper]](https://www.aclweb.org/anthology/P15-1152.pdf), [[bibtex]](https://www.aclweb.org/anthology/P15-1152.bib).
- [2016 AAAI] **Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models**, [[paper]](https://arxiv.org/pdf/1507.04808.pdf), [[bibtex]](/Bibtex/Building%20End-To-End%20Dialogue%20Systems%20Using%20Generative%20Hierarchical%20Neural%20Network%20Models.bib), sources: [[suriyadeepan/augmented_seq2seq]](https://github.com/suriyadeepan/augmented_seq2seq), [[julianser/hed-dlg]](https://github.com/julianser/hed-dlg), [[sordonia/hed-dlg]](https://github.com/sordonia/hed-dlg), [[julianser/hred-latent-piecewise]](https://github.com/julianser/hred-latent-piecewise), [[julianser/hed-dlg-truncated]](https://github.com/julianser/hed-dlg-truncated).
- [2016 ACL] **On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems**, [[paper]](https://www.aclweb.org/anthology/P16-1230.pdf), [[bibtex]](https://www.aclweb.org/anthology/P16-1230.bib).
- [2016 EMNLP] **Deep Reinforcement Learning for Dialogue Generation**, [[paper]](https://www.aclweb.org/anthology/D16-1127.pdf), [[bibtex]](https://www.aclweb.org/anthology/D16-1127.bib), sources: [[liuyuemaicha/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow]](https://github.com/liuyuemaicha/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow).
- [2016 EMNLP] **Multi-view Response Selection for Human-Computer Conversation**, [[paper]](https://www.aclweb.org/anthology/D16-1036.pdf), [[bibtex]](https://www.aclweb.org/anthology/D16-1036.bib).
- [2017 KDD Explorations Newsletter] **A Survey on Dialogue Systems: Recent Advances and New Frontiers**, [[paper]](https://arxiv.org/pdf/1711.01731.pdf), [[bibtex]](/Bibtex/A%20Survey%20on%20Dialogue%20Systems.bib), sources: [[shawnspace/survey-in-dialog-system]](https://github.com/shawnspace/survey-in-dialog-system).
- [2017 EMNLP] **Adversarial Learning for Neural Dialogue Generation**, [[paper]](https://www.aclweb.org/anthology/D17-1230.pdf), [[bibtex]](https://www.aclweb.org/anthology/D17-1230.bib), sources: [[jiweil/Neural-Dialogue-Generation]](https://github.com/jiweil/Neural-Dialogue-Generation), [[liuyuemaicha/Adversarial-Learning-for-Neural-Dialogue-Generation-in-Tensorflow]](https://github.com/liuyuemaicha/Adversarial-Learning-for-Neural-Dialogue-Generation-in-Tensorflow).
- [2017 ACL] **Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots**, [[paper]](https://www.aclweb.org/anthology/P17-1046.pdf), [[bibtex]](https://www.aclweb.org/anthology/P17-1046.bib), sources: [[MarkWuNLP/MultiTurnResponseSelection]](https://github.com/MarkWuNLP/MultiTurnResponseSelection), [[krayush07/sequential-match-network]](https://github.com/krayush07/sequential-match-network).
- [2018 COLING] **Sequence-to-sequence Data Augmentation for Dialogue Language Understanding**, [[paper]](http://aclweb.org/anthology/C18-1105), [[bibtex]](/Bibtex/Sequence-to-sequence%20Data%20Augmentation%20for%20Dialogue%20Language%20Understanding.bib), sources: [[AtmaHou/Seq2SeqDataAugmentationForLU]](https://github.com/AtmaHou/Seq2SeqDataAugmentationForLU).
- [2018 IEEE] **Neural Approaches to Conversational AI: Question Answering, Task-Oriented Dialogues and Social Chatbots**, [[paper]](https://arxiv.org/pdf/1809.08267.pdf), [[slides]](https://www.microsoft.com/en-us/research/uploads/prod/2018/07/neural-approaches-to-conversational-AI.pdf), [[bibtex]](/Bibtex/Neural%20Approaches%20to%20Conversational%20AI.bib).
- [2018 EMNLP] **Spherical Latent Spaces for Stable Variational Autoencoders**, [[paper]](https://aclweb.org/anthology/D18-1480), [[bibtex]](/Bibtex/Spherical%20Latent%20Spaces%20for%20Stable%20Variational%20Autoencoders.bib), sources: [[jiacheng-xu/vmf_vae_nlp]](https://github.com/jiacheng-xu/vmf_vae_nlp).
- [2019 ACL] **Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models**, [[paper]](https://www.aclweb.org/anthology/P19-1200), [[bibtex]](/Bibtex/Towards%20Generating%20Long%20and%20Coherent%20Text%20with%20Multi-Level%20Latent%20Variable%20Models.bib).
- [2019 ACL] **Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems**, [[paper]](https://www.aclweb.org/anthology/P19-1078.pdf), [[bibtex]](/Bibtex/Transferable%20Multi-Domain%20State%20Generator%20for%20Task-Oriented%20Dialogue%20Systems.bib), sources: [[jasonwu0731/trade-dst]](https://github.com/jasonwu0731/trade-dst).
- [2019 IJCAI] **Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset**, [[paper]](https://arxiv.org/pdf/1909.05855.pdf), [[bibtex]](/Bibtex/Towards%20Scalable%20Multi-domain%20Conversational%20Agents%20-%20The%20Schema-Guided%20Dialogue%20Dataset.bib), [[slides]](https://scai.info/ijcai2019/slides/IJCAI2019_Zang.pdf).
- [2020 NeurIPS] **Zero-Resource Knowledge-Grounded Dialogue Generation**, [[paper]](https://papers.nips.cc/paper/2020/file/609c5e5089a9aa967232aba2a4d03114-Paper.pdf), [[bibtex]](/Bibtex/Zero-Resource%20Knowledge-Grounded%20Dialogue%20Generation.bib), sources: [[nlpxucan/ZRKGC]](https://github.com/nlpxucan/ZRKGC).

## Text Summarization
- [2016 ACL] **Incorporating Copying Mechanism in Sequence-to-Sequence Learning**, [[paper]](Incorporating%20Copying%20Mechanism%20in%20Sequence-to-Sequence%20Learning), [[bibtex]](/Bibtex/Incorporating%20Copying%20Mechanism%20in%20Sequence-to-Sequence%20Learning.bib), sources: [[mjc92/CopyNet]](https://github.com/mjc92/CopyNet), [[lspvic/CopyNet]](https://github.com/lspvic/CopyNet), [[MultiPath/CopyNet]](https://github.com/MultiPath/CopyNet), [[google/text2text]](https://github.com/google/text2text), [[jaredwei01/CopyNet]](https://github.com/jaredwei01/CopyNet).
- [2017 ACL] **Get To The Point: Summarization with Pointer-Generator Networks**, [[paper]](http://aclweb.org/anthology/P17-1099), [[notes]](http://anthology.aclweb.org/attachments/P/P17/P17-1099.Notes.pdf), [[slides]](http://anthology.aclweb.org/attachments/P/P17/P17-1099.Presentation.pdf), [[bibtex]](/Bibtex/Get%20To%20The%20Point%20-%20Summarization%20with%20Pointer-Generator%20Networks.bib), sources: [[abisee/pointer-generator]](https://github.com/abisee/pointer-generator), [[abisee/cnn-dailymail]](https://github.com/abisee/cnn-dailymail), [[JafferWilson/Process-Data-of-CNN-DailyMail]](https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail).
- [2018 COLING] **Structure-Infused Copy Mechanisms for Abstractive Summarization**, [[paper]](http://aclweb.org/anthology/C18-1146), [[bibtex]](/Bibtex/Structure-Infused%20Copy%20Mechanisms%20for%20Abstractive%20Summarization.bib), sources: [[KaiQiangSong/struct_infused_summ]](https://github.com/KaiQiangSong/struct_infused_summ).
- [2018 ICLR] **Generating Wikipedia by Summarizing Long Sequences**, [[paper]](https://openreview.net/pdf?id=Hyg0vbWC-), [[bibtex]](/Bibtex/Generating%20Wikipedia%20by%20Summarizing%20Long%20Sequences.bib), sources: [[tensorflow/tensor2tensor · wikisum]](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum).
- [2018 ACML] **A Self-Attentive Hierarchical Model for Jointly Improving Text Summarization and Sentiment Classification**, [[paper]](http://proceedings.mlr.press/v95/wang18b/wang18b.pdf), [[bibtex]](/Bibtex/A%20Self-Attentive%20Hierarchical%20Model%20for%20Jointly%20Improving%20Text%20Summarization%20and%20Sentiment%20Classification.bib).
- [2019 ArXiv] **On Extractive and Abstractive Neural Document Summarization with Transformer Language Models**, [[paper]](https://arxiv.org/pdf/1909.03186.pdf), [[bibtex]](/Bibtex/On%20Extractive%20and%20Abstractive%20Neural%20Document%20Summarization.bib).