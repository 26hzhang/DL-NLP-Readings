# Visual Representation Learning

## Visual Representation Learning on Image
- [2018 ArXiv] **Representation Learning with Contrastive Predictive Coding**, [[paper]](https://arxiv.org/pdf/1807.03748.pdf), [[bibtex]](/Bibtex/Representation%20Learning%20with%20Contrastive%20Predictive%20Coding.bib), sources: [[davidtellez/contrastive-predictive-coding]](https://github.com/davidtellez/contrastive-predictive-coding), [[flrngel/cpc-tensorflow]](https://github.com/flrngel/cpc-tensorflow), [[jefflai108/Contrastive-Predictive-Coding-PyTorch]](https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch).
- [2018 CVPR] **Unsupervised Feature Learning via Non-Parametric Instance Discrimination**, [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0801.pdf), [[bibtex]](/Bibtex/Unsupervised%20Feature%20Learning%20via%20Non-Parametric%20Instance%20Discrimination.bib), sources: [[zhirongw/lemniscate.pytorch]](https://github.com/zhirongw/lemniscate.pytorch).
- [2019 CVPR] **Revisiting Self-Supervised Visual Representation Learning**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Revisiting%20Self-Supervised%20Visual%20Representation%20Learning.bib), sources: [[google/revisiting-self-supervised]](https://github.com/google/revisiting-self-supervised).
- [2020 CVPR] **Momentum Contrast for Unsupervised Visual Representation Learning**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Momentum%20Contrast%20for%20Unsupervised%20Visual%20Representation%20Learning.bib), sources: [[facebookresearch/moco]](https://github.com/facebookresearch/moco).
- [2020 CVPR] **Visual Commonsense R-CNN**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Visual_Commonsense_R-CNN_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Visual%20Commonsense%20R-CNN.bib), sources: [[Wangt-CN/VC-R-CNN]](https://github.com/Wangt-CN/VC-R-CNN).
- [2020 ICML] **Generative Pretraining from Pixels**, [[paper]](http://proceedings.mlr.press/v119/chen20s/chen20s.pdf), [[bibtex]](/Bibtex/Generative%20Pretraining%20from%20Pixels.bib), sources: [[openai/image-gpt]](https://github.com/openai/image-gpt).
- [2020 NeurIPS] **Big Self-Supervised Models are Strong Semi-Supervised Learners**, [[paper]](https://papers.nips.cc/paper/2020/file/fcbc95ccdd551da181207c0c1400c655-Paper.pdf), [[bibtex]](/Bibtex/Big%20Self-Supervised%20Models%20are%20Strong%20Semi-Supervised%20Learners.bib), sources: [[google-research/simclr]](https://github.com/google-research/simclr).
- [2020 NeurIPS] **Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning**, [[paper]](https://papers.nips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf), [[bibtex]](/Bibtex/Bootstrap%20Your%20Own%20Latent%20A%20New%20Approach%20to%20Self-Supervised%20Learning.bib), sources: [[lucidrains/byol-pytorch]](https://github.com/lucidrains/byol-pytorch).
- [2020 NeurIPS] **Unsupervised Learning of Visual Features by Contrasting Cluster Assignments**, [[paper]](https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf), [[bibtex]](/Bibtex/Unsupervised%20Learning%20of%20Visual%20Features%20by%20Contrasting%20Cluster%20Assignments.bib), sources: [[facebookresearch/swav]](https://github.com/facebookresearch/swav).
- [2020 ECCV] **Big Transfer (BiT): General Visual Representation Learning**, [[paper]](https://arxiv.org/pdf/1912.11370.pdf), [[bibtex]](/Bibtex/Big%20Transfer%20BiT%20-%20General%20Visual%20Representation%20Learning.bib), sources: [[google-research/big_transfer]](https://github.com/google-research/big_transfer).
- [2020 ICML] **A Simple Framework for Contrastive Learning of Visual Representations**, [[paper]](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf), [[bibtex]](/Bibtex/A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations.bib), sources: [[google-research/simclr]](https://github.com/google-research/simclr).
- [2021 ICLR] **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**, [[paper]](https://openreview.net/pdf?id=YicbFdNTTy), [[bibtex]](/Bibtex/An%20Image%20is%20Worth%2016x16%20Words%20-%20Transformers%20for%20Image%20Recognition%20at%20Scale.bib), sources: [[google-research/vision_transformer]](https://github.com/google-research/vision_transformer).
- [2021 ICLR] **Representation Learning via Invariant Causal Mechanisms**, [[paper]](https://openreview.net/pdf?id=9p2ekP904Rs), [[bibtex]](/Bibtex/Representation%20Learning%20via%20Invariant%20Causal%20Mechanisms.bib).
- [2021 ArXiv] **Conditional Positional Encodings for Vision Transformers**, [[paper]](https://arxiv.org/pdf/2102.10882v2.pdf), [[bibtex]](/Bibtex/Conditional%20Positional%20Encodings%20for%20Vision%20Transformers.bib), sources: [[Meituan-AutoML/CPVT]](https://github.com/Meituan-AutoML/CPVT).
- [2021 ICML] **Training Data-Efficient Image Transformers & Distillation Through Attention**, [[paper]](http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf), [[bibtex]](/Bibtex/Training%20data-efficient%20image%20transformers%20&%20distillation%20through%20attention.bib), [[supplementary]](http://proceedings.mlr.press/v139/touvron21a/touvron21a-supp.pdf), sources: [[facebookresearch/deit]](https://github.com/facebookresearch/deit).
- [2021 ArXiv] **An Attention Free Transformer**, [[paper]](https://arxiv.org/pdf/2105.14103.pdf), [[bibtex]](/Bibtex/An%20Attention%20Free%20Transformer.bib), sources: [[rish-16/aft-pytorch]](https://github.com/rish-16/aft-pytorch).
- [2021 ArXiv] **Proactive Pseudo-Intervention: Contrastive Learning For Interpretable Vision Models**, [[paper]](https://arxiv.org/pdf/2012.03369.pdf), [[bibtex]](/Bibtex/Proactive%20Pseudo-Intervention%20-%20Contrastive%20Learning%20For%20Interpretable%20Vision%20Models.bib).
- [2021 ArXiv] **Adversarial Visual Robustness by Causal Intervention**, [[paper]](https://arxiv.org/pdf/2106.09534.pdf), [[bibtex]](/Bibtex/Adversarial%20Visual%20Robustness%20by%20Causal%20Intervention.bib), sources: [[KaihuaTang/Adversarial-Robustness-by-Causal-Intervention.pytorch]](https://github.com/KaihuaTang/Adversarial-Robustness-by-Causal-Intervention.pytorch).
- [2021 CVPR] **Evolving Attention with Residual Convolutions**, [[paper]](https://arxiv.org/pdf/2102.12895.pdf), [[bibtex]](/Bibtex/Evolving%20Attention%20with%20Residual%20Convolutions.bib).
- [2021 CVPR] **Exploring Simple Siamese Representation Learning**, [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.pdf), [[bibtex]](/Bibtex/Exploring%20Simple%20Siamese%20Representation%20Learning.bib), sources: [[facebookresearch/simsiam]](https://github.com/facebookresearch/simsiam).
- [2021 CVPR] **Pre-Trained Image Processing Transformer**, [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.pdf), [[bibtex]](/Bibtex/Pre-Trained%20Image%20Processing%20Transformer.bib), [[supplementary]](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Chen_Pre-Trained_Image_Processing_CVPR_2021_supplemental.pdf), sources: [[huawei-noah/Pretrained-IPT]](https://github.com/huawei-noah/Pretrained-IPT), [[]]().
- [2021 ICML] **Understanding Self-Supervised Learning Dynamics without Contrastive Pairs**, [[paper]](https://research.fb.com/wp-content/uploads/2021/06/Understanding-self-supervised-Learning-Dynamics-without-Contrastive-Pairs.pdf), [[bibtex]](/Bibtex/Understanding%20Self-Supervised%20Learning%20Dynamics%20without%20Contrastive%20Pairs.bib), [[slides]](https://icml.cc/media/icml-2021/Slides/10403.pdf), sources: [[facebookresearch/luckmatters/ssl]](https://github.com/facebookresearch/luckmatters/tree/master/ssl).
- [2021 ArXiv] **Masked Autoencoders Are Scalable Vision Learners**, [[paper]](https://arxiv.org/pdf/2111.06377.pdf), [[bibtex]](/Bibtex/Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners.bib), sources: [[facebookresearch/mae]](https://github.com/facebookresearch/mae), [[pengzhiliang/MAE-pytorch]](https://github.com/pengzhiliang/MAE-pytorch).
- [2022 ICLR] **BEiT: BERT Pre-Training of Image Transformers**, [[paper]](https://openreview.net/pdf?id=p-BhZSz59o4), [[bibtex]](/Bibtex/BEIT%20-%20BERT%20Pre-Training%20of%20Image%20Transformers.bib), sources: [[microsoft/beit]](https://github.com/microsoft/unilm/tree/master/beit), [[huggingface/transformers/beit]](https://github.com/huggingface/transformers/tree/master/src/transformers/models/beit).
- [2022 ICLR] **SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption**, [[paper]](https://openreview.net/pdf?id=CuV_qYkmKb3), [[bibtex]](/Bibtex/SCARF%20-%20Self-Supervised%20Contrastive%20Learning%20using%20Random%20Feature%20Corruption.bib).
- [2022 ArXiv] **data2vec: A General Framework for Self-supervised Learning in Speech, Vision
and Language**, [[paper]](https://arxiv.org/pdf/2202.03555.pdf), [[bibtex]](/Bibtex/data2vec.bib), sources: [[pytorch/fairseq/data2vec]](https://github.com/pytorch/fairseq/tree/main/examples/data2vec).

## Visual Representation Learning on Video
- [2019 ICCV] **Video Representation Learning by Dense Predictive Coding**, [[paper]](https://arxiv.org/pdf/1909.04656.pdf), [[bibtex]](/Bibtex/Video%20Representation%20Learning%20by%20Dense%20Predictive%20Coding.bib), sources: [[TengdaHan/DPC]](https://github.com/TengdaHan/DPC).
- [2021 ArXiv] **TCLR: Temporal Contrastive Learning for Video Representation**, [[paper]](https://arxiv.org/pdf/2101.07974.pdf), [[bibtex]](/Bibtex/Temporal%20Contrastive%20Learning%20for%20Video%20Representation.bib).
- [2021 ArXiv] **Self-supervised Video Retrieval Transformer Network**, [[paper]](https://arxiv.org/pdf/2104.07993.pdf), [[bibtex]](/Bibtex/Self-supervised%20Video%20Retrieval%20Transformer%20Network.bib).
- [2021 CVPR] **The Blessings of Unlabeled Background in Untrimmed Videos**, [[paper]](https://arxiv.org/pdf/2103.13183.pdf), [[bibtex]](/Bibtex/The%20Blessings%20of%20Unlabeled%20Background%20in%20Untrimmed%20Videos.bib), sources: [[liuyuancv/WTAL_blessing]](https://github.com/liuyuancv/WTAL_blessing).