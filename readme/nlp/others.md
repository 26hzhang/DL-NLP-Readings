# Other NLP Research Works

> Including **NLP Survey**, **Optimizing Methods in NLP**, **Grammatical Error Correction**, **Code Generation**, **Recurrent Neural Network**, **Multi-task Learning** and etc.

## Natural Language Survey
- [2018 JAIR] **Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation**, [[paper]](https://arxiv.org/pdf/1703.09902.pdf), [[bibtex]](/Bibtex/Survey%20of%20the%20State%20of%20the%20Art%20in%20Natural%20Language%20Generation.bib).
- [2018 CIM] **Recent Trends in Deep Learning Based Natural Language Processing**, [[paper]](https://arxiv.org/pdf/1708.02709.pdf), [[bibtex]](/Bibtex/Recent%20Trends%20in%20Deep%20Learning%20Based%20Natural%20Language%20Processing.bib).
- [2020 ArXiv] **Low-resource Languages: A Review of Past Work and Future Challenges**, [[paper]](https://arxiv.org/pdf/2006.07264.pdf), [[bibtex]](/Bibtex/Low-resource%20Languages%20-%20A%20Review%20of%20Past%20Work%20and%20Future%20Challenges.bib).

## Code Generation
- [2017 ESEC/FSE] **Are Deep Neural Networks the Best Choice for Modeling Source Code?**, [[paper]](http://web.cs.ucdavis.edu/~devanbu/isDLgood.pdf), [[bibtex]](/Bibtex/Are%20Deep%20Neural%20Networks%20the%20Best%20Choice%20for%20Modeling%20Source%20Code.bib).
- [2018 EMNLP] **Mapping Language to Code in Programmatic Context**, [[paper]](https://aclweb.org/anthology/D18-1192), [[bibtex]](/Bibtex/Mapping%20Language%20to%20Code%20in%20Programmatic%20Context.bib), sources: [[sriniiyer/concode]](https://github.com/sriniiyer/concode).
- [2019 ArXiv] **Maybe Deep Neural Networks are the Best Choice for Modeling Source Code**, [[paper]](https://arxiv.org/pdf/1903.05734.pdf), [[bibtex]](/Bibtex/Maybe%20Deep%20Neural%20Networks%20are%20the%20Best%20Choice%20for%20Modeling%20Source%20Code.bib), [[slides]](https://research.jetbrains.org/files/material/5ce3172d8cfcc.pdf), sources: [[mast-group/OpenVocabCodeNLM]](https://github.com/mast-group/OpenVocabCodeNLM).
- [2019 ArXiv] **Neural Networks for Modeling Source Code Edits**, [[paper]](https://arxiv.org/pdf/1904.02818.pdf), [[bibtex]](/Bibtex/Neural%20Networks%20for%20Modeling%20Source%20Code%20Edits.bib).

## Grammatical Error Correction
- [2014 CoNLL] **The CoNLL-2014 Shared Task on Grammatical Error Correction**, [[paper]](http://www.aclweb.org/anthology/W14-1701), [[bibtex]](/Bibtex/The%20CoNLL-2014%20Shared%20Task%20on%20Grammatical%20Error%20Correction.bib) [[homepage]](http://www.comp.nus.edu.sg/~nlp/conll14st.html).
- [2018 AAAI] **A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction**, [[paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17308/16137), [[bibtex]](/Bibtex/A%20Multilayer%20Convolutional%20Encoder-Decoder%20Neural%20Network%20for%20Grammatical%20Error%20Correction.bib), [[nusnlp/mlconvgec2018]](https://github.com/nusnlp/mlconvgec2018).
- [2018 NAACL] **Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation**, [[paper]](https://www.aclweb.org/anthology/N18-2046.pdf), [[bibtex]](/Bibtex/Near%20Human-Level%20Performance%20in%20Grammatical%20Error%20Correction%20with%20Hybrid%20Machine%20Translation.bib).
- [2019 NAACL] **Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data**, [[paper]](https://www.aclweb.org/anthology/N19-1014), [[bibtex]](/Bibtex/Improving%20Grammatical%20Error%20Correction%20via%20Pre-Training%20aCopy-Augmented%20Architecture%20with%20Unlabeled%20Data.bib), sources: [[zhawe01/fairseq-gec]](https://github.com/zhawe01/fairseq-gec).

## Multi-task and Transfer Learning
- [2011 JMLR] **Natural Language Processing (Almost) from Scratch**, [[paper]](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf), [[bibtex]](/Bibtex/Natural%20Language%20Processing%20%28Almost%29%20from%20Scratch.bib), sources: [[attardi/deepnl]](https://github.com/attardi/deepnl).
- [2016 EMNLP] **How Transferable are Neural Networks in NLP Applications?**, [[paper]](https://www.aclweb.org/anthology/D16-1046.pdf), [[bibtex]](https://www.aclweb.org/anthology/D16-1046.bib).
- [2017 EMNLP] **A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks**, [[paper]](http://aclweb.org/anthology/D17-1206), [[bibtex]](/Bibtex/A%20Joint%20Many-Task%20Model%20-%20Growing%20a%20Neural%20Network%20for%20Multiple%20NLP%20Tasks.bib), [[blog]](https://theneuralperspective.com/2017/03/08/a-joint-many-task-model-growing-a-neural-network-for-multiple-nlp-tasks/), sources: [[rubythonode/joint-many-task-model]](https://github.com/rubythonode/joint-many-task-model), [[hassyGo/charNgram2vec]](https://github.com/hassyGo/charNgram2vec).
- [2019 AAAI] **Latent Multi-task Architecture Learning** or **Sluice Networks: Learning What to Share Between Loosely Related Tasks**, [[paper]](https://www.aaai.org/Papers/AAAI/2019/AAAI-RuderS.6318.pdf), [[bibtex]](/Bibtex/Latent%20Multi-task%20Architecture%20Learning.bib), sources: [[sebastianruder/sluice-networks]](https://github.com/sebastianruder/sluice-networks).
- [2019 NAACL] **Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together**, [[paper]](https://www.aclweb.org/anthology/N19-1127.pdf), [[bibtex]](/Bibtex/Tensorized%20Self-Attention%20-%20Efficiently%20Modeling%20Pairwise%20and%20Global%20Dependencies%20Together.bib), sources: [[taoshen58/mtsa]](https://github.com/taoshen58/mtsa).
- [2019 ACL] **Choosing Transfer Languages for Cross-Lingual Learning**, [[paper]](https://www.aclweb.org/anthology/P19-1301), [[bibtex]](Choosing%20Transfer%20Languages%20for%20Cross-Lingual%20Learning.bib), sources: [[neulab/langrank]](https://github.com/neulab/langrank).

## Others
- [2018 ArXiv] **Fast Directional Self-Attention Mechanism**, [[paper]](https://arxiv.org/pdf/1805.00912.pdf), [[bibtex]](/Bibtex/Fast%20Directional%20Self-Attention%20Mechanism.bib), sources: [[taoshen58/Fast-DiSA]](https://github.com/taoshen58/DiSAN/tree/master/Fast-DiSA).
- [2019 NAACL] **Attention is not Explanation**, [[paper]](https://www.aclweb.org/anthology/N19-1357.pdf), [[bibtex]](/Bibtex/Attention%20is%20not%20Explanation.bib), sources: [[successar/AttentionExplanation]](https://github.com/successar/AttentionExplanation).
- [2019 EMNLP] **Attention is not not Explanation**, [[paper]](https://arxiv.org/pdf/1908.04626.pdf), [[bibtex]](/Bibtex/Attention%20is%20not%20not%20Explanation.bib), sources: [[sarahwie/attention]](https://github.com/sarahwie/attention).
- [2019 ACL] **Augmenting Neural Networks with First-order Logic**, [[paper]](https://www.aclweb.org/anthology/P19-1028.pdf), [[bibtex]](/Bibtex/Augmenting%20Neural%20Networks%20with%20First-order%20Logic.bib), sources: [[utahnlp/layer_augmentation]](https://github.com/utahnlp/layer_augmentation).
- [2020 ICLR] **BERTScore: Evaluating Text Generation with BERT**, [[paper]](https://openreview.net/pdf?id=SkeHuCVFDr), [[bibtex]](/Bibtex/BERTScore%20-%20Evaluating%20Text%20Generation%20with%20BERT.bib), sources: [[Tiiiger/bert_score]](https://github.com/Tiiiger/bert_score).
- [2021 ArXiv] **TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing**, [[paper]](https://arxiv.org/pdf/2103.11441.pdf), [[bibtex]](/Bibtex/TextFlint%20-%20Unified%20Multilingual%20Robustness%20Evaluation%20Toolkit%20for%20Natural%20Language%20Processing.bib), sources: [[textflint/textflint]](https://github.com/textflint/textflint).
