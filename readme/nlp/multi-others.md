# Multi-tasks and Others

## Natural Language Survey
- [2018 JAIR] **Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation**, [[paper]](https://arxiv.org/pdf/1703.09902.pdf).
- [2018 CIM] **Recent Trends in Deep Learning Based Natural Language Processing**, [[paper]](https://arxiv.org/pdf/1708.02709.pdf).

## Others
- [2016 EMNLP] **How Transferable are Neural Networks in NLP Applications?**, [[paper]](http://www.aclweb.org/anthology/D16-1046).
- [2017 ACL] **Learning When to Skim and When to Read**, [[paper]](http://www.aclweb.org/anthology/W17-2631), [[blog]](https://einstein.ai/research/learning-when-to-skim-and-when-to-read).
- [2017 ICLR] **An Actor Critic Algorithm for Structured Prediction**, [[paper]](https://openreview.net/pdf?id=SJDaqqveg), [[bibtex]](/Bibtex/An%20Actor%20Critic%20Algorithm%20for%20Structured%20Prediction.bib), sources: [[rizar/actor-critic-public]](https://github.com/rizar/actor-critic-public).
- [2017 ICML] **Language Modeling with Gated Convolutional Networks**, [[paper]](https://arxiv.org/pdf/1612.08083.pdf), sources: [[anantzoid/Language-Modeling-GatedCNN]](https://github.com/anantzoid/Language-Modeling-GatedCNN), [[jojonki/Gated-Convolutional-Networks]](https://github.com/jojonki/Gated-Convolutional-Networks).
- [2018 ICLR] **Regularizing and Optimizing LSTM Language Models**, [[paper]](https://openreview.net/pdf?id=SyyGPP0TZ), [[bibtex]](/Bibtex/Regularizing%20and%20Optimizing%20LSTM%20Language%20Models.bib), sources: [[salesforce/awd-lstm-lm]](https://github.com/salesforce/awd-lstm-lm), author page: [[Nitish Shirish Keskar]](https://keskarnitish.github.io).
- [2018 ArXiv] **GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations**, [[paper]](https://arxiv.org/pdf/1806.05662.pdf), [[bibtex]](GLoMo%20-%20Unsupervisedly%20Learned%20Relational%20Graphs%20as%20Transferable%20Representations.bib).
- [2018 ArXiv] **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**, [[paper]](https://arxiv.org/pdf/1810.04805.pdf), [[bibtex]](/Bibtex/BERT%20-%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.bib).
- [2018 ArXiv] **Fast Directional Self-Attention Mechanism**, [[paper]](https://arxiv.org/pdf/1805.00912.pdf), [[bibtex]](/Bibtex/Fast%20Directional%20Self-Attention%20Mechanism.bib), sources: [[taoshen58/Fast-DiSA]](https://github.com/taoshen58/DiSAN/tree/master/Fast-DiSA).
- [2018 ICLR] **Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling**, [[paper]](https://openreview.net/pdf?id=H1cWzoxA-), [[bibtex]](/Bibtex/Bi-Directional%20Block%20Self-Attention%20for%20Fast%20and%20Memory-Efficient%20Sequence%20Modeling.bib), sources: [[taoshen58/BiBloSA]](https://github.com/taoshen58/BiBloSA).