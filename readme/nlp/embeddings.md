# Embeddings, Interpretability and Disambiguation

## Char Embedding
- [2016 AAAI] **Char2Vec: Character-Aware Neural Language Models**, [[paper]](https://arxiv.org/pdf/1508.06615.pdf), sources: [[carpedm20/lstm-char-cnn-tensorflow]](https://github.com/carpedm20/lstm-char-cnn-tensorflow), [[yoonkim/lstm-char-cnn]](https://github.com/yoonkim/lstm-char-cnn).

## Word Embedding
- [2008 NIPS] **HLBL: A Scalable Hierarchical Distributed Language Model**, [[paper]](http://www.cs.toronto.edu/~fritz/absps/andriytree.pdf), [[wenjieguan/Log-bilinear-language-models]](https://github.com/wenjieguan/Log-bilinear-language-models).
- [2010 INTERSPEECH] **RNNLM: Recurrent neural network based language model**, [[paper]](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf), [[Ph.D. Thesis]](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf), [[slides]](http://www.fit.vutbr.cz/~imikolov/rnnlm/google.pdf), sources: [[mspandit/rnnlm]](https://github.com/mspandit/rnnlm).
- [2013 NIPS] **Word2Vec: Distributed Representations of Words and Phrases and their Compositionality**, [[paper]](https://arxiv.org/pdf/1310.4546.pdf), [[word2vec explained]](https://arxiv.org/pdf/1402.3722.pdf), [[params explained]](https://arxiv.org/pdf/1411.2738.pdf), [[blog]](https://isaacchanghau.github.io/post/word2vec/), sources: [[word2vec]](https://code.google.com/archive/p/word2vec/), [[dav/word2vec]](https://github.com/dav/word2vec), [[yandex/faster-rnnlm]](https://github.com/yandex/faster-rnnlm), [[tf-word2vec]](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/word2vec), [[zake7749/word2vec-tutorial]](https://github.com/zake7749/word2vec-tutorial).
- [2013 CoNLL] **Better Word Representations with Recursive Neural Networks for Morphology**, [[paper]](https://nlp.stanford.edu/~lmthang/data/papers/conll13_morpho.pdf).
- [2014 ACL] **Word2Vecf: Dependency-Based Word Embeddings**, [[paper]](http://www.aclweb.org/anthology/P14-2050), [[blog]](https://isaacchanghau.github.io/post/word2vecf/), sources: [[Yoav Goldberg/word2vecf]](https://bitbucket.org/yoavgo/word2vecf), [[IsaacChanghau/Word2VecfJava]](https://github.com/IsaacChanghau/Word2VecfJava).
- [2014 EMNLP] **GloVe: Global Vectors for Word Representation**, [[paper]](https://nlp.stanford.edu/pubs/glove.pdf), [[homepage]](https://nlp.stanford.edu/projects/glove/), sources: [[stanfordnlp/GloVe]](https://github.com/stanfordnlp/GloVe).
- [2014 ICML] **Compositional Morphology for Word Representations and Language Modelling**, [[paper]](http://proceedings.mlr.press/v32/botha14.pdf), sources: [[thompsonb/comp-morph]](https://github.com/thompsonb/comp-morph), [[claravania/subword-lstm-lm]](https://github.com/claravania/subword-lstm-lm).
- [2015 ACL] **Hyperword: Improving Distributional Similarity with Lessons Learned from Word Embeddings**, [[paper]](http://www.aclweb.org/anthology/Q15-1016), sources: [[Omer Levy/hyperwords]](https://bitbucket.org/omerlevy/hyperwords).
- [2016 ICLR] **Exploring the Limits of Language Modeling**, [[paper]](https://arxiv.org/pdf/1602.02410.pdf), [[slides]](https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/lipnet.pdf), sources: [[tensorflow/models/lm_1b]](https://github.com/tensorflow/models/tree/master/research/lm_1b).
- [2016 CoNLL] **Context2Vec: Learning Generic Context Embedding with Bidirectional LSTM**, [[paper]](http://www.aclweb.org/anthology/K16-1006), sources: [[orenmel/context2vec]](https://github.com/orenmel/context2vec).
- [2016 IEEE Intelligent Systems] **How to Generate a Good Word Embedding?**, [[paper]](https://arxiv.org/pdf/1507.05523.pdf), [[基于神经网络的词和文档语义向量表示方法研究]](https://arxiv.org/pdf/1611.05962.pdf), [[blog]](http://licstar.net/archives/620), sources: [[licstar/compare]](https://github.com/licstar/compare).
- [2016 ArXiv] **Linear Algebraic Structure of Word Senses, with Applications to Polysemy**, [[paper]](https://arxiv.org/pdf/1601.03764.pdf), [[slides]](https://pdfs.semanticscholar.org/d770/5adf01fc9791337ed17dd37236129ef3a0f4.pdf), sources: [[YingyuLiang/SemanticVector]](https://github.com/YingyuLiang/SemanticVector).
- [2017 ACL] **FastText: Enriching Word Vectors with Subword Information**, [[paper]](https://arxiv.org/pdf/1607.04606.pdf), sources: [[facebookresearch/fastText]](https://github.com/facebookresearch/fastText), [[salestock/fastText.py]](https://github.com/salestock/fastText.py).
- [2017 ICLR] **A Simple But Tough-to-beat Baseline for Sentence Embeddings**, [[paper]](https://openreview.net/pdf?id=SyK00v5xx), sources: [[PrincetonML/SIF]](https://github.com/PrincetonML/SIF).
- [2017 NIPS] **Learned in Translation: Contextualized Word Vectors**, [[paper]](https://arxiv.org/pdf/1708.00107.pdf), sources: [[salesforce/cove]](https://github.com/salesforce/cove).
- [2017 ArXiv] **Implicitly Incorporating Morphological Information into Word Embedding**, [[paper]](https://arxiv.org/pdf/1701.02481.pdf).
- [2017 AAAI] **Improving Word Embeddings with Convolutional Feature Learning and Subword Information**, [[paper]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14724/14187), sources: [[ShelsonCao/IWE]](https://github.com/ShelsonCao/IWE).
- [2018 NAACL] **Deep contextualized word representations**, [[paper]](https://arxiv.org/pdf/1802.05365.pdf), [[homepage]](https://allennlp.org/elmo), sources: [[allenai/bilm-tf]](https://github.com/allenai/bilm-tf).
- [2018 ICML] **Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations**, [[paper]](https://arxiv.org/pdf/1806.09464.pdf), [supplementary](http://web.cs.ucla.edu/~yzsun/papers/2018_icml_KDCoding_supp.pdf), sources: [[chentingpc/kdcode-lm]](https://github.com/chentingpc/kdcode-lm).

## Sentence Embedding and Representation
- [2017 ICLR] **A Structured Self-attentive Sentence Embedding**, [[paper]](https://arxiv.org/pdf/1703.03130.pdf), sources: [[ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding]](https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding), [[flrngel/Self-Attentive-tensorflow]](https://github.com/flrngel/Self-Attentive-tensorflow), [[kaushalshetty/Structured-Self-Attention]](https://github.com/kaushalshetty/Structured-Self-Attention).

## Interpretability
- [2012 COLING] **Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding**, [[paper]](http://talukdar.net/papers/nnse_coling12.pdf), [[homepage]](http://www.cs.cmu.edu/~bmurphy/NNSE/).
- [2015 NAACL] **A Compositional and Interpretable Semantic Space**, [[paper]](http://www.aclweb.org/anthology/N15-1004).
- [2015 EMNLP] **Online Learning of Interpretable Word Embeddings**, [[paper]](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/emnlp2015_word2nvec.pdf).
- [2015 ACL] **SPOWV: Sparse Overcomplete Word Vector Representations**, [[paper]](http://www.aclweb.org/anthology/P15-1144), [[mfaruqui/sparse-coding]](https://github.com/mfaruqui/sparse-coding).
- [2016 IJCAI] **Sparse Word Embeddings Using l1 Regularized Online Learning**, [[paper]](https://www.ijcai.org/Proceedings/16/Papers/414.pdf), [[slides]](http://ofey.me/slides/sparse_ijcai16.pdf).
- [2017 ArXiv] **Semantic Structure and Interpretability of Word Embeddings**, [[paper]](https://arxiv.org/pdf/1711.00331.pdf).
- [2017 EMNLP] **Rotated Word Vector Representations and their Interpretability**, [[paper]](http://www.aclweb.org/anthology/D17-1041), [[poster]](https://sungjoonpark.github.io./assets/emnlp2017_poster.pdf), sources: [[SungjoonPark/factor_rotation]](https://github.com/SungjoonPark/factor_rotation), [[mvds314/factor_rotation]](https://github.com/mvds314/factor_rotation).
- [2018 AAAI] **SPINE: SParse Interpretable Neural Embeddings**, [[paper]](https://arxiv.org/pdf/1711.08792.pdf), sources: [[harsh19/SPINE]](https://github.com/harsh19/SPINE).
- [2018 ACL] **Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder**, [[paper]](), [[bibtex]](/Bibtex/Interpretable%20and%20Compositional%20Relation%20Learning%20by%20Joint%20Training%20with%20an%20Autoencoder.bib), sources: [[tianran/glimvec]](https://github.com/tianran/glimvec).

## Disambiguation
- [2015 VSM] **A Simple Word Embedding Model for Lexical Substitution**, [[paper]](http://u.cs.biu.ac.il/%7Emelamuo/publications/melamud_vsm15.pdf), sources: [[orenmel/lexsub]](https://github.com/orenmel/lexsub).
- [2017 EMNLP] **Deep Joint Entity Disambiguation with Local Neural Attention**, [[paper]](http://aclweb.org/anthology/D17-1277), sources: [[dalab/deep-ed]](https://github.com/dalab/deep-ed).
