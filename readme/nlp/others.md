# Other NLP Research Works

> Including **NLP Survey**, **Optimizing Methods in NLP**, **Grammatical Error Correction**, **Code Generation**, **Recurrent Neural Network**, **Multi-task Learning** and etc.

## Natural Language Survey
- [2018 JAIR] **Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation**, [[paper]](https://arxiv.org/pdf/1703.09902.pdf), [[bibtex]](/Bibtex/Survey%20of%20the%20State%20of%20the%20Art%20in%20Natural%20Language%20Generation.bib).
- [2018 CIM] **Recent Trends in Deep Learning Based Natural Language Processing**, [[paper]](https://arxiv.org/pdf/1708.02709.pdf), [[bibtex]](/Bibtex/Recent%20Trends%20in%20Deep%20Learning%20Based%20Natural%20Language%20Processing.bib).
- [2020 ArXiv] **Low-resource Languages: A Review of Past Work and Future Challenges**, [[paper]](https://arxiv.org/pdf/2006.07264.pdf), [[bibtex]](/Bibtex/Low-resource%20Languages%20-%20A%20Review%20of%20Past%20Work%20and%20Future%20Challenges.bib).

## Code Generation
- [2017 ESEC/FSE] **Are Deep Neural Networks the Best Choice for Modeling Source Code?**, [[paper]](http://web.cs.ucdavis.edu/~devanbu/isDLgood.pdf), [[bibtex]](/Bibtex/Are%20Deep%20Neural%20Networks%20the%20Best%20Choice%20for%20Modeling%20Source%20Code.bib).
- [2018 EMNLP] **Mapping Language to Code in Programmatic Context**, [[paper]](https://aclweb.org/anthology/D18-1192), [[bibtex]](/Bibtex/Mapping%20Language%20to%20Code%20in%20Programmatic%20Context.bib), sources: [[sriniiyer/concode]](https://github.com/sriniiyer/concode).
- [2019 ArXiv] **Maybe Deep Neural Networks are the Best Choice for Modeling Source Code**, [[paper]](https://arxiv.org/pdf/1903.05734.pdf), [[bibtex]](/Bibtex/Maybe%20Deep%20Neural%20Networks%20are%20the%20Best%20Choice%20for%20Modeling%20Source%20Code.bib), [[slides]](https://research.jetbrains.org/files/material/5ce3172d8cfcc.pdf), sources: [[mast-group/OpenVocabCodeNLM]](https://github.com/mast-group/OpenVocabCodeNLM).
- [2019 ArXiv] **Neural Networks for Modeling Source Code Edits**, [[paper]](https://arxiv.org/pdf/1904.02818.pdf), [[bibtex]](/Bibtex/Neural%20Networks%20for%20Modeling%20Source%20Code%20Edits.bib).

## Grammatical Error Correction
- [2014 CoNLL] **The CoNLL-2014 Shared Task on Grammatical Error Correction**, [[paper]](http://www.aclweb.org/anthology/W14-1701), [[bibtex]](/Bibtex/The%20CoNLL-2014%20Shared%20Task%20on%20Grammatical%20Error%20Correction.bib) [[homepage]](http://www.comp.nus.edu.sg/~nlp/conll14st.html).
- [2018 AAAI] **A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction**, [[paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17308/16137), [[bibtex]](/Bibtex/A%20Multilayer%20Convolutional%20Encoder-Decoder%20Neural%20Network%20for%20Grammatical%20Error%20Correction.bib), [[nusnlp/mlconvgec2018]](https://github.com/nusnlp/mlconvgec2018).
- [2018 NAACL] **Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation**, [[paper]](https://www.aclweb.org/anthology/N18-2046.pdf), [[bibtex]](/Bibtex/Near%20Human-Level%20Performance%20in%20Grammatical%20Error%20Correction%20with%20Hybrid%20Machine%20Translation.bib).
- [2019 NAACL] **Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data**, [[paper]](https://www.aclweb.org/anthology/N19-1014), [[bibtex]](/Bibtex/Improving%20Grammatical%20Error%20Correction%20via%20Pre-Training%20aCopy-Augmented%20Architecture%20with%20Unlabeled%20Data.bib), sources: [[zhawe01/fairseq-gec]](https://github.com/zhawe01/fairseq-gec).

## Multi-task and Transfer Learning
- [2011 JMLR] **Natural Language Processing (Almost) from Scratch**, [[paper]](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf), [[bibtex]](/Bibtex/Natural%20Language%20Processing%20%28Almost%29%20from%20Scratch.bib), sources: [[attardi/deepnl]](https://github.com/attardi/deepnl).
- [2016 EMNLP] **How Transferable are Neural Networks in NLP Applications?**, [[paper]](https://www.aclweb.org/anthology/D16-1046.pdf), [[bibtex]](https://www.aclweb.org/anthology/D16-1046.bib).
- [2017 EMNLP] **A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks**, [[paper]](http://aclweb.org/anthology/D17-1206), [[bibtex]](/Bibtex/A%20Joint%20Many-Task%20Model%20-%20Growing%20a%20Neural%20Network%20for%20Multiple%20NLP%20Tasks.bib), [[blog]](https://theneuralperspective.com/2017/03/08/a-joint-many-task-model-growing-a-neural-network-for-multiple-nlp-tasks/), sources: [[rubythonode/joint-many-task-model]](https://github.com/rubythonode/joint-many-task-model), [[hassyGo/charNgram2vec]](https://github.com/hassyGo/charNgram2vec).
- [2019 AAAI] **Latent Multi-task Architecture Learning** or **Sluice Networks: Learning What to Share Between Loosely Related Tasks**, [[paper]](https://www.aaai.org/Papers/AAAI/2019/AAAI-RuderS.6318.pdf), [[bibtex]](/Bibtex/Latent%20Multi-task%20Architecture%20Learning.bib), sources: [[sebastianruder/sluice-networks]](https://github.com/sebastianruder/sluice-networks).
- [2019 NAACL] **Tensorized Self-Attention - Efficiently Modeling Pairwise and Global Dependencies Together**, [[paper]](https://www.aclweb.org/anthology/N19-1127.pdf), [[bibtex]](/Bibtex/Tensorized%20Self-Attention%20-%20Efficiently%20Modeling%20Pairwise%20and%20Global%20Dependencies%20Together.bib), sources: [[taoshen58/mtsa]](https://github.com/taoshen58/mtsa).
- [2019 ACL] **Choosing Transfer Languages for Cross-Lingual Learning**, [[paper]](https://www.aclweb.org/anthology/P19-1301), [[bibtex]](Choosing%20Transfer%20Languages%20for%20Cross-Lingual%20Learning.bib), sources: [[neulab/langrank]](https://github.com/neulab/langrank).

## Recurrent Neural Network
- [2001 PhD Thesis] **Long Short-Term Memory in Recurrent Neural Networks**, [[Gers' Ph.D. Thesis]](https://www.researchgate.net/profile/Felix_Gers/publication/2562741_Long_Short-Term_Memory_in_Recurrent_Neural_Networks/links/5759410a08ae9a9c954e77f5.pdf), [[bibtex]](/Bibtex/Long%20Short-Term%20Memory%20in%20Recurrent%20Neural%20Networks.bib).
- [2014 ArXiv] **Recurrent Neural Network Regularization**, [[paper]](https://arxiv.org/abs/1409.2329), [[bibtex]](/Bibtex/Recurrent%20Neural%20Network%20Regularization.bib).
- [2015 ArXiv] **Grid Long Short-Term Memory**, [[paper]](https://arxiv.org/abs/1507.01526), [[bibtex]](/Bibtex/Grid%20Long%20Short-Term%20Memory.bib), sources: [[Tensotflow-GridLSTMCell]](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/GridLSTMCell).
- [2016 ArXiv] **Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks**, [[paper]](https://arxiv.org/abs/1611.06204), [[bibtex]](/Bibtex/Visualizing%20and%20Understanding%20Curriculum%20Learning%20for%20Long%20Short-Term%20Memory%20Networks.bib).
- [2016 ArXiv] **Contextual LSTM (CLSTM) models for Large scale NLP tasks**, [[paper]](https://arxiv.org/pdf/1602.06291v2.pdf), [[bibtex]](/Bibtex/Contextual%20LSTM%20models%20for%20Large%20scale%20NLP%20tasks.bib), sources: [[kafkasl/contextualLSTM]](https://github.com/kafkasl/contextualLSTM).
- [2016 ICLR] **Visualizing and Understanding Recurrent Networks**, [[paper]](http://vision.stanford.edu/pdf/KarpathyICLR2016.pdf), [[bibtex]](/Bibtex/Visualizing%20and%20Understanding%20Recurrent%20Networks.bib)
- [2016 NIPS] **Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences**, [[paper]](https://arxiv.org/pdf/1610.09513v1.pdf), [[bibtex]](/Bibtex/Phased%20LSTM.bib), sources: [[Tensorflow-PhasedLSTMCell]](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/PhasedLSTMCell).
- [2016 NIPS] **A Theoretically Grounded Application of Dropout in Recurrent Neural Networks**, [[paper]](https://arxiv.org/pdf/1512.05287.pdf), [[bibtex]](/Bibtex/A%20Theoretically%20Grounded%20Application%20of%20Dropout%20in%20Recurrent%20Neural%20Networks.bib), sources: [[DropoutWrapper TF]](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper).
- [2017 ACML] **Nested LSTMs**, [[paper]](https://arxiv.org/abs/1801.10308), [[bibtex]](/Bibtex/Nested%20LSTMs.bib), sources: [[hannw/nlstm]](https://github.com/hannw/nlstm), [[titu1994/Nested-LSTM]](https://github.com/titu1994/Nested-LSTM).
- [2017 ICLR] **Variable Computation in Recurrent Neural Networks**, [[paper]](https://arxiv.org/pdf/1611.06188.pdf), [[bibtex]](/Bibtex/Variable%20Computation%20in%20Recurrent%20Neural%20Networks.bib).
- [2018 EMNLP] **Simple Recurrent Units for Highly Parallelizable Recurrence**, [[paper]](http://aclweb.org/anthology/D18-1477), [[bibtex]](/Bibtex/Simple%20Recurrent%20Units%20for%20Highly%20Parallelizable%20Recurrence.bib), sources: [[taolei87/sru]](https://github.com/taolei87/sru).
- [2018 ICLR] **Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks**, [[paper]](https://openreview.net/pdf?id=HkwVAXyCW), [[bibtex]](/Bibtex/Skip%20RNN.bib), [[homepage]](https://imatge-upc.github.io/skiprnn-2017-telecombcn/), sources: [[imatge-upc/skiprnn-2017-telecombcn]](https://github.com/imatge-upc/skiprnn-2017-telecombcn).
- [2019 ArXiv] **Single Headed Attention RNN: Stop Thinking With Your Head**, [[paper]](https://arxiv.org/pdf/1911.11423.pdf), [[bibtex]](/Bibtex/Single%20Headed%20Attention%20RNN%20-%20Stop%20Thinking%20With%20Your%20Head.bib), sources: [[Smerity/sha-rnn]](https://github.com/Smerity/sha-rnn).

## Others
- [2018 ArXiv] **Fast Directional Self-Attention Mechanism**, [[paper]](https://arxiv.org/pdf/1805.00912.pdf), [[bibtex]](/Bibtex/Fast%20Directional%20Self-Attention%20Mechanism.bib), sources: [[taoshen58/Fast-DiSA]](https://github.com/taoshen58/DiSAN/tree/master/Fast-DiSA).
- [2019 NAACL] **Attention is not Explanation**, [[paper]](https://www.aclweb.org/anthology/N19-1357.pdf), [[bibtex]](/Bibtex/Attention%20is%20not%20Explanation.bib), sources: [[successar/AttentionExplanation]](https://github.com/successar/AttentionExplanation).
- [2019 EMNLP] **Attention is not not Explanation**, [[paper]](https://arxiv.org/pdf/1908.04626.pdf), [[bibtex]](/Bibtex/Attention%20is%20not%20not%20Explanation.bib), sources: [[sarahwie/attention]](https://github.com/sarahwie/attention).
- [2019 ACL] **Augmenting Neural Networks with First-order Logic**, [[paper]](https://www.aclweb.org/anthology/P19-1028.pdf), [[bibtex]](/Bibtex/Augmenting%20Neural%20Networks%20with%20First-order%20Logic.bib), sources: [[utahnlp/layer_augmentation]](https://github.com/utahnlp/layer_augmentation).
- [2020 ICLR] **BERTScore: Evaluating Text Generation with BERT**, [[paper]](https://openreview.net/pdf?id=SkeHuCVFDr), [[bibtex]](/Bibtex/BERTScore%20-%20Evaluating%20Text%20Generation%20with%20BERT.bib), sources: [[Tiiiger/bert_score]](https://github.com/Tiiiger/bert_score).
- [2021 ArXiv] **TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing**, [[paper]](https://arxiv.org/pdf/2103.11441.pdf), [[bibtex]](/Bibtex/TextFlint%20-%20Unified%20Multilingual%20Robustness%20Evaluation%20Toolkit%20for%20Natural%20Language%20Processing.bib), sources: [[textflint/textflint]](https://github.com/textflint/textflint).
