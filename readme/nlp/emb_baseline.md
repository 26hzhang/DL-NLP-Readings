# Char/Word Embeddings and Basline Systems

> Including **Character Embeddings**, **Word Embeddings** and **Baseline Systems**.

## Character Embeddings
- [2016 AAAI] **Char2Vec: Character-Aware Neural Language Models**, [[paper]](https://arxiv.org/pdf/1508.06615.pdf), sources: [[carpedm20/lstm-char-cnn-tensorflow]](https://github.com/carpedm20/lstm-char-cnn-tensorflow), [[yoonkim/lstm-char-cnn]](https://github.com/yoonkim/lstm-char-cnn).

## Word Embeddings
- [2008 NIPS] **HLBL: A Scalable Hierarchical Distributed Language Model**, [[paper]](http://www.cs.toronto.edu/~fritz/absps/andriytree.pdf), [[wenjieguan/Log-bilinear-language-models]](https://github.com/wenjieguan/Log-bilinear-language-models).
- [2010 INTERSPEECH] **RNNLM: Recurrent neural network based language model**, [[paper]](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf), [[Ph.D. Thesis]](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf), [[slides]](http://www.fit.vutbr.cz/~imikolov/rnnlm/google.pdf), sources: [[mspandit/rnnlm]](https://github.com/mspandit/rnnlm).
- [2013 NIPS] **Word2Vec: Distributed Representations of Words and Phrases and their Compositionality**, [[paper]](https://arxiv.org/pdf/1310.4546.pdf), [[word2vec explained]](https://arxiv.org/pdf/1402.3722.pdf), [[params explained]](https://arxiv.org/pdf/1411.2738.pdf), [[blog]](https://isaacchanghau.github.io/post/word2vec/), sources: [[word2vec]](https://code.google.com/archive/p/word2vec/), [[dav/word2vec]](https://github.com/dav/word2vec), [[yandex/faster-rnnlm]](https://github.com/yandex/faster-rnnlm), [[tf-word2vec]](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/word2vec), [[zake7749/word2vec-tutorial]](https://github.com/zake7749/word2vec-tutorial).
- [2013 CoNLL] **Better Word Representations with Recursive Neural Networks for Morphology**, [[paper]](https://nlp.stanford.edu/~lmthang/data/papers/conll13_morpho.pdf).
- [2014 ACL] **Word2Vecf: Dependency-Based Word Embeddings**, [[paper]](http://www.aclweb.org/anthology/P14-2050), [[blog]](https://isaacchanghau.github.io/post/word2vecf/), sources: [[Yoav Goldberg/word2vecf]](https://bitbucket.org/yoavgo/word2vecf), [[IsaacChanghau/Word2VecfJava]](https://github.com/IsaacChanghau/Word2VecfJava).
- [2014 EMNLP] **GloVe: Global Vectors for Word Representation**, [[paper]](https://nlp.stanford.edu/pubs/glove.pdf), [[homepage]](https://nlp.stanford.edu/projects/glove/), sources: [[stanfordnlp/GloVe]](https://github.com/stanfordnlp/GloVe).
- [2014 ICML] **Compositional Morphology for Word Representations and Language Modelling**, [[paper]](http://proceedings.mlr.press/v32/botha14.pdf), sources: [[thompsonb/comp-morph]](https://github.com/thompsonb/comp-morph), [[claravania/subword-lstm-lm]](https://github.com/claravania/subword-lstm-lm).
- [2015 ACL] **Hyperword: Improving Distributional Similarity with Lessons Learned from Word Embeddings**, [[paper]](http://www.aclweb.org/anthology/Q15-1016), sources: [[Omer Levy/hyperwords]](https://bitbucket.org/omerlevy/hyperwords).
- [2016 ICLR] **Exploring the Limits of Language Modeling**, [[paper]](https://arxiv.org/pdf/1602.02410.pdf), [[slides]](https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/lipnet.pdf), sources: [[tensorflow/models/lm_1b]](https://github.com/tensorflow/models/tree/master/research/lm_1b).
- [2016 CoNLL] **Context2Vec: Learning Generic Context Embedding with Bidirectional LSTM**, [[paper]](http://www.aclweb.org/anthology/K16-1006), sources: [[orenmel/context2vec]](https://github.com/orenmel/context2vec).
- [2016 IEEE Intelligent Systems] **How to Generate a Good Word Embedding?**, [[paper]](https://arxiv.org/pdf/1507.05523.pdf), [[基于神经网络的词和文档语义向量表示方法研究]](https://arxiv.org/pdf/1611.05962.pdf), [[blog]](http://licstar.net/archives/620), sources: [[licstar/compare]](https://github.com/licstar/compare).
- [2016 ArXiv] **Linear Algebraic Structure of Word Senses, with Applications to Polysemy**, [[paper]](https://arxiv.org/pdf/1601.03764.pdf), [[slides]](https://pdfs.semanticscholar.org/d770/5adf01fc9791337ed17dd37236129ef3a0f4.pdf), sources: [[YingyuLiang/SemanticVector]](https://github.com/YingyuLiang/SemanticVector).
- [2017 ACL] **FastText: Enriching Word Vectors with Subword Information**, [[paper]](https://arxiv.org/pdf/1607.04606.pdf), sources: [[facebookresearch/fastText]](https://github.com/facebookresearch/fastText), [[salestock/fastText.py]](https://github.com/salestock/fastText.py).
- [2017 ArXiv] **Implicitly Incorporating Morphological Information into Word Embedding**, [[paper]](https://arxiv.org/pdf/1701.02481.pdf).
- [2017 AAAI] **Improving Word Embeddings with Convolutional Feature Learning and Subword Information**, [[paper]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14724/14187), sources: [[ShelsonCao/IWE]](https://github.com/ShelsonCao/IWE).
- [2018 ICML] **Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations**, [[paper]](https://arxiv.org/pdf/1806.09464.pdf), [supplementary](http://web.cs.ucla.edu/~yzsun/papers/2018_icml_KDCoding_supp.pdf), sources: [[chentingpc/kdcode-lm]](https://github.com/chentingpc/kdcode-lm).
- [2018 ICLR] **Compressing Word Embeddings via Deep Compositional Code Learning**, [[paper]](https://openreview.net/pdf?id=BJRZzFlRb), [[bibtex]](/Bibtex/Compressing%20Word%20Embeddings%20via%20Deep%20Compositional%20Code%20Learning.bib), sources: [[msobroza/compositional_code_learning]](https://github.com/msobroza/compositional_code_learning).

## Baseline Systems
- [2017 NIPS] **Learned in Translation: Contextualized Word Vectors**, [[paper]](https://arxiv.org/pdf/1708.00107.pdf), sources: [[salesforce/cove]](https://github.com/salesforce/cove).
- [2018 NAACL] **Deep contextualized word representations**, [[paper]](https://arxiv.org/pdf/1802.05365.pdf), [[homepage]](https://allennlp.org/elmo), sources: [[allenai/bilm-tf]](https://github.com/allenai/bilm-tf).
- [2018 ArXiv] **GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations**, [[paper]](https://arxiv.org/pdf/1806.05662.pdf), [[bibtex]](GLoMo%20-%20Unsupervisedly%20Learned%20Relational%20Graphs%20as%20Transferable%20Representations.bib).
- [2018 ArXiv] **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**, [[paper]](https://arxiv.org/pdf/1810.04805.pdf), [[bibtex]](/Bibtex/BERT%20-%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.bib), sources: [[google-research/bert]](https://github.com/google-research/bert), [[huggingface/pytorch-pretrained-BERT]](https://github.com/huggingface/pytorch-pretrained-BERT).