# Video Retrieval and Captioning

## Video-level Retrieval
- [2015 AAAI] **Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework**, [[paper]](https://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf), [[bibtex]](/Bibtex/Jointly%20Modeling%20Deep%20Video%20and%20Compositional%20Text%20to%20Bridge%20Vision%20and%20Language%20in%20a%20Unified%20Framework.bib).
- [2018 ECCV] **Find and Focus: Retrieve and Localize Video Events with Natural Language Queries**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Dian_SHAO_Find_and_Focus_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Find%20and%20Focus%20-%20Retrieve%20and%20Localize%20Video%20Events%20with%20Natural%20Language%20Queries.bib).
- [2018 ECCV] **Cross-Modal and Hierarchical Modeling of Video and Text**, [[paper]](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Bowen_Zhang_Cross-Modal_and_Hierarchical_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Cross-Modal%20and%20Hierarchical%20Modeling%20of%20Video%20and%20Text.bib), sources: [[zbwglory/CMHSE]](https://github.com/zbwglory/CMHSE).
- [2019 CVPR] **Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Song_Polysemous_Visual-Semantic_Embedding_for_Cross-Modal_Retrieval_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Polysemous%20Visual-Semantic%20Embedding%20for%20Cross-Modal%20Retrieval.bib), sources: [[yalesong/pvse]](https://github.com/yalesong/pvse).
- [2020 IEEE TM] **SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries**, [[paper]](https://arxiv.org/pdf/2011.12091.pdf), [[bibtex]](/Bibtex/SEA%20-%20Sentence%20Encoder%20Assembly%20for%20Video%20Retrieval%20by%20Textual%20Queries.bib).

## Video Captioning
- [2015 ICCV] **Sequence to Sequence: Video to Text**, [[paper]](http://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf), [[bibtex]](/Bibtex/Sequence%20to%20Sequence%20â€“%20Video%20to%20Text.bib), [[homepage]](https://vsubhashini.github.io/s2vt.html), sources: [[vsubhashini/caffe/examples/s2vt]](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt).
- [2017 ICCV] **Dense-Captioning Events in Videos**, [[paper]](https://arxiv.org/pdf/1705.00754.pdf), [[bibtex]](/Bibtex/Dense-Captioning%20Events%20in%20Videos.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/densevid/), source: [[ranjaykrishna/densevid_eval]](https://github.com/ranjaykrishna/densevid_eval).
- [2017 ArXiv] **Multi-Task Video Captioning with Video and Entailment Generation**, [[paper]](https://arxiv.org/pdf/1704.07489.pdf), [[bibtex]](/Bibtex/Multi-Task%20Video%20Captioning%20with%20Video%20and%20Entailment%20Generation.bib).
- [2018 CVPR] **Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning**, [[paper]](https://arxiv.org/pdf/1804.00100.pdf), [[bibtex]](/Bibtex/Bidirectional%20Attentive%20Fusion%20with%20Context%20Gating%20for%20Dense%20Video%20Captioning.bib), sources: [[JaywongWang/DenseVideoCaptioning]](https://github.com/JaywongWang/DenseVideoCaptioning).
- [2018 CVPR] **End-to-End Dense Video Captioning with Masked Transformer**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/End-to-End%20Dense%20Video%20Captioning%20with%20Masked%20Transformer.bib), sources: [[salesforce/densecap]](https://github.com/salesforce/densecap).
- [2018 CVPR] **Finding It: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos**, [[paper]](http://vision.stanford.edu/pdf/huang-buch-2018cvpr), [[bibtex]](/Bibtex/Finding%20It%20-%20Weakly-Supervised%20Reference-Aware%20Visual%20Grounding%20in%20Instructional%20Videos.bib), [[supplementary]](https://finding-it.github.io/finding-it-suppmat.pdf), [[poster]](https://drive.google.com/file/d/1uvnw6VDn0r1nS3ePyFKaCbEx5GZw1ZEy/view), [[homepage]](https://finding-it.github.io), [[youtube]](https://www.youtube.com/watch?v=GBo4sFNzhtU&feature=youtu.be&t=1366).
- [2018 NeurIPS] **Weakly Supervised Dense Event Captioning in Videos**, [[paper]](https://papers.nips.cc/paper/7569-weakly-supervised-dense-event-captioning-in-videos.pdf), [[bibtex]](/Bibtex/Weakly%20Supervised%20Dense%20Event%20Captioning%20in%20Videos.bib), sources: [[XgDuan/WSDEC]](https://github.com/XgDuan/WSDEC).
- [2019 WACV] **Joint Event Detection and Description in Continuous Video Streams**, [[paper]](http://www.boyangli.co/paper/huijuanxu-wacv-2019.pdf), [[bibtex]](/Bibtex/Joint%20Event%20Detection%20and%20Description%20in%20Continuous%20Video%20Streams.bib), sources: [[VisionLearningGroup/JEDDi-Net]](https://github.com/VisionLearningGroup/JEDDi-Net).
- [2019 CVPR] **Grounded Video Description**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Grounded%20Video%20Description.bib), sources: [[facebookresearch/ActivityNet-Entities]](https://github.com/facebookresearch/ActivityNet-Entities), [[facebookresearch/grounded-video-description]](https://github.com/facebookresearch/grounded-video-description).
- [2019 CSUR] **Video Description: A Survey of Methods, Datasets, and Evaluation Metrics**, [[paper]](https://arxiv.org/pdf/1806.00186.pdf), [[bibtex]](/Bibtex/Video%20Description%20-%20A%20Survey%20of%20Methods%20Datasets%20and%20Evaluation%20Metrics.bib).
- [2019 ACL] **Dense Procedure Captioning in Narrated Instructional Videos**, [[paper]](https://www.aclweb.org/anthology/P19-1641.pdf), [[bibtex]](/Bibtex/Dense%20Procedure%20Captioning%20in%20Narrated%20Instructional%20Videos.bib).
- [2019 ACL] **Multimodal Abstractive Summarization for How2 Videos**, [[paper]](https://www.aclweb.org/anthology/P19-1659.pdf), [[bibtex]](/Bibtex/Multimodal%20Abstractive%20Summarization%20for%20How2%20Videos.bib).
- [2019 EMNLP] **Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag**, [[paper]](https://www.aclweb.org/anthology/D19-1213.pdf), [[bibtex]](https://www.aclweb.org/anthology/D19-1213.bib).
- [2019 ICCV] **Watch, Listen and Tell - Multi-modal Weakly Supervised Dense Event Captioning**, [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Rahman_Watch_Listen_and_Tell_Multi-Modal_Weakly_Supervised_Dense_Event_Captioning_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Watch%20Listen%20and%20Tell%20-%20Multi-modal%20Weakly%20Supervised%20Dense%20Event%20Captioning.bib).
- [2020 ICCV] **VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/VATEX.bib), [[homepage]](http://vatex.org/main/index.html).
- [2020 ACL] **MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning**, [[paper]](https://www.aclweb.org/anthology/2020.acl-main.233.pdf), [[bibtex]](/Bibtex/MART.bib), sources: [[jayleicn/recurrent-transformer]](https://github.com/jayleicn/recurrent-transformer).