# Image-based VQA, Visual Reasoning and Dialogue


## Visual Question Answering and Visual Reasoning
- [2015 ICCV] **VQA: Visual Question Answering**, [[paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf), [[bibtex]](/Bibtex/VQA%20-%20Visual%20Question%20Answering.bib), [[homepage]](https://visualqa.org/index.html).
- [2016 NIPS] **Hierarchical Question-Image Co-Attention for Visual Question Answering**, [[paper]](https://arxiv.org/pdf/1606.00061), [[bibtex]](/Bibtex/Hierarchical%20Question-Image%20Co-Attention%20for%20Visual%20Question%20Answering.bib), sources: [[karunraju/VQA]](https://github.com/karunraju/VQA), [[jiasenlu/HieCoAttenVQA]](https://github.com/jiasenlu/HieCoAttenVQA).
- [2016 ICML] **Dynamic Memory Networks for Visual and Textual Question Answering**, [[paper]](http://proceedings.mlr.press/v48/xiong16.pdf), [[bibtex]](/Bibtex/Dynamic%20Memory%20Networks%20for%20Visual%20and%20Textual%20Question%20Answering.bib), [[blog]](https://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/), sources: [[therne/dmn-tensorflow]](https://github.com/therne/dmn-tensorflow), [[barronalex/Dynamic-Memory-Networks-in-TensorFlow]](https://github.com/barronalex/Dynamic-Memory-Networks-in-TensorFlow), [[ethancaballero/Improved-Dynamic-Memory-Networks-DMN-plus]](https://github.com/ethancaballero/Improved-Dynamic-Memory-Networks-DMN-plus), [[dandelin/Dynamic-memory-networks-plus-Pytorch]](https://github.com/dandelin/Dynamic-memory-networks-plus-Pytorch), [[DeepRNN/visual_question_answering]](https://github.com/DeepRNN/visual_question_answering).
- [2016 CVPR] **Stacked Attention Networks for Image Question Answering**, [[paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf), [[bibtex]](/Bibtex/Stacked%20Attention%20Networks%20for%20Image%20Question%20Answering.bib), sources: [[zcyang/imageqa-san]](https://github.com/zcyang/imageqa-san).
- [2016 CVPR] **Neural Module Networks**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2016/papers/Andreas_Neural_Module_Networks_CVPR_2016_paper.pdf), [[bibtex]](/Bibtex/Neural%20Module%20Networks.bib).
- [2016 EMNLP] **Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding**, [[paper]](https://www.aclweb.org/anthology/D16-1044.pdf), [[bibtex]](https://www.aclweb.org/anthology/D16-1044.bib), sources: [[akirafukui/vqa-mcb]](https://github.com/akirafukui/vqa-mcb), [[Cadene/vqa.pytorch]](https://github.com/Cadene/vqa.pytorch), [[MarcBS/keras]](https://github.com/MarcBS/keras).
- [2017 CVPR] **CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning**, [[paper]](https://openaccess.thecvf.com/content_cvpr_2017/papers/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/CLEVR%20-%20A%20Diagnostic%20Dataset%20for%20Compositional%20Language%20and%20Elementary%20Visual%20Reasoning.bib), sources: [[facebookresearch/clevr-dataset-gen]](https://github.com/facebookresearch/clevr-dataset-gen).
- [2018 ECCV] **Visual Question Answering as a Meta Learning Task**, [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/papers/Damien_Teney_Visual_Question_Answering_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Visual%20Question%20Answering%20as%20a%20Meta%20Learning%20Task.bib).
- [2018 CVPR] **Visual Grounding via Accumulated Attention**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Visual%20Grounding%20via%20Accumulated%20Attention.bib).
- [2018 CVPR] **Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Bottom-Up%20and%20Top-Down%20Attention%20for%20Image%20Captioning%20and%20Visual%20Question%20Answering.bib), sources: [[peteanderson80/bottom-up-attention]](https://github.com/peteanderson80/bottom-up-attention), [[hengyuan-hu/bottom-up-attention-vqa]](https://github.com/hengyuan-hu/bottom-up-attention-vqa), [[LeeDoYup/bottom-up-attention-tf]](https://github.com/LeeDoYup/bottom-up-attention-tf).
- [2019 AAAI] **Dynamic Capsule Attention for Visual Question Answering**, [[paper]](/Documents/Papers/Dynamic%20Capsule%20Attention%20for%20Visual%20Question%20Answering.pdf), [[bibtex]](/Bibtex/Dynamic%20Capsule%20Attention%20for%20Visual%20Question%20Answering.bib).
- [2019 AAAI] **BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection**, [[paper]](https://arxiv.org/pdf/1902.00038.pdf), [[bibtex]](/Bibtex/Block.bib), sources: [[Cadene/block.bootstrap.pytorch]](https://github.com/Cadene/block.bootstrap.pytorch).
- [2019 ACL] **Multi-grained Attention with Object-level Grounding for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/P19-1349.pdf), [[bibtex]](/Bibtex/Multi-grained%20Attention%20with%20Object-level%20Grounding%20for%20Visual%20Question%20Answering.bib).
- [2019 EMNLP] **B2T2: Fusion of Detected Objects in Text for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/D19-1219.pdf), [[bibtex]](/Bibtex/Fusion%20of%20Detected%20Objects%20in%20Text%20for%20Visual%20Question%20Answering.bib), sources: [[google-research/language/language/question_answering/b2t2/]](https://github.com/google-research/language/tree/master/language/question_answering/b2t2).
- [2019 ICCV] **Multi-modality Latent Interaction Network for Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Multi-Modality_Latent_Interaction_Network_for_Visual_Question_Answering_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Multi-modality%20Latent%20Interaction%20Network%20for%20Visual%20Question%20Answering.bib).
- [2019 CVPR] **Towards VQA Models That Can Read**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Towards%20VQA%20Models%20That%20Can%20Read.bib), sources: [[facebookresearch/pythia]](https://github.com/facebookresearch/pythia).
- [2019 CVPR] **Learning to Compose Dynamic Tree Structures for Visual Contexts**, [[paper]](https://zpascal.net/cvpr2019/Tang_Learning_to_Compose_Dynamic_Tree_Structures_for_Visual_Contexts_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Learning%20to%20Compose%20Dynamic%20Tree%20Structures%20for%20Visual%20Contexts.bib), sources: [[KaihuaTang/VCTree-Scene-Graph-Generation]](https://github.com/KaihuaTang/VCTree-Scene-Graph-Generation).
- [2019 CVPR] **GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/GQA.bib), [[homepage]](visualreasoning.net).
- [2020 AAAI] **ManyModalQA: Modality Disambiguation and QA over Diverse Inputs**, [[paper]](https://arxiv.org/pdf/2001.08034.pdf), [[bibtex]](/Bibtex/ManyModalQA.bib), sources: [[hannandarryl/ManyModalQA]](https://github.com/hannandarryl/ManyModalQA).
- [2020 ACL] **Multimodal Neural Graph Memory Networks for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/2020.acl-main.643.pdf), [[bibtex]](/Bibtex/Multimodal%20Neural%20Graph%20Memory%20Networks%20for%20Visual%20Question%20Answering.bib).
- [2020 CVPR] **VQA with No Questions-Answers Training**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Vatashsky_VQA_With_No_Questions-Answers_Training_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/VQA%20with%20No%20Questions-Answers%20Training.bib), sources: [[benyv/uncord]](https://github.com/benyv/uncord).

## Bias in Visual Question Answering
- [2017 CVPR] **Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering**, [[paper]](https://openaccess.thecvf.com/content_cvpr_2017/papers/Goyal_Making_the_v_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/Making%20the%20V%20in%20VQA%20Matter%20-%20Elevating%20the%20Role%20of%20Image%20Understanding%20in%20Visual%20Question%20Answering.bib), [[homepage]](https://visualqa.org/).
- [2017 ICCV] **Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization**, [[paper]](https://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/Grad-CAM%20-%20Visual%20Explanations%20from%20Deep%20Networks%20via%20Gradient-based%20Localization.bib), sources: [[ramprs/grad-cam]](https://github.com/ramprs/grad-cam/).
- [2018 CVPR] **Donâ€™t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering**, [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Agrawal_Dont_Just_Assume_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Dont%20Just%20Assume%20Look%20and%20Answer%20-%20Overcoming%20Priors%20for%20Visual%20Question%20Answering.bib), [[homepage]](https://www.cc.gatech.edu/~aagrawal307/vqa-cp/), sources: [[AishwaryaAgrawal/GVQA]](https://github.com/AishwaryaAgrawal/GVQA).
- [2018 NeurIPS] **Overcoming Language Priors in Visual Question Answering with Adversarial Regularization**, [[paper]](http://papers.nips.cc/paper/7427-overcoming-language-priors-in-visual-question-answering-with-adversarial-regularization.pdf), [[bibtex]](/Bibtex/Overcoming%20Language%20Priors%20in%20Visual%20Question%20Answering%20with%20Adversarial%20Regularization.bib).
- [2019 NAACL] **Adversarial Regularization for Visual Question Answering: Strengths Shortcomings and Side Effects**, [[paper]](https://www.aclweb.org/anthology/W19-1801.pdf), [[bibtex]](https://www.aclweb.org/anthology/W19-1801.bib).
- [2019 SIGIR] **Quantifying and Alleviating the Language Prior Problem in Visual Question Answering**, [[paper]](https://arxiv.org/pdf/1905.04877.pdf), [[bibtex]](/Bibtex/Quantifying%20and%20Alleviating%20the%20Language%20Prior%20Problem%20in%20Visual%20Question%20Answering.bib), sources: [[guoyang9/vqa-prior]](https://github.com/guoyang9/vqa-prior).
- [2019 EMNLP] **Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases**, [[paper]](https://www.aclweb.org/anthology/D19-1418.pdf), [[bibtex]](https://www.aclweb.org/anthology/D19-1418.bib), sources: [[chrisc36/debias]](https://github.com/chrisc36/debias).
- [2019 CVPR] **Explicit Bias Discovery in Visual Question Answering Models**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Manjunatha_Explicit_Bias_Discovery_in_Visual_Question_Answering_Models_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Explicit%20Bias%20Discovery%20in%20Visual%20Question%20Answering%20Models.bib).
- [2019 NeurIPS] **RUBi: Reducing Unimodal Biases for Visual Question Answering**, [[paper]](http://papers.nips.cc/paper/8371-rubi-reducing-unimodal-biases-for-visual-question-answering.pdf), [[bibtex]](/Bibtex/RUBi%20-%20Reducing%20Unimodal%20Biases%20for%20Visual%20Question%20Answering.bib), sources: [[cdancette/rubi.bootstrap.pytorch]](https://github.com/cdancette/rubi.bootstrap.pytorch).
- [2019 NeurIPS] **Self-Critical Reasoning for Robust Visual Question Answering**, [[paper]](https://papers.nips.cc/paper/2019/file/33b879e7ab79f56af1e88359f9314a10-Paper.pdf), [[bibtex]](/Bibtex/Self-Critical%20Reasoning%20for%20Robust%20Visual%20Question%20Answering.bib), sources: [[jialinwu17/self_critical_vqa]](https://github.com/jialinwu17/self_critical_vqa).
- [2019 ICCV] **Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded**, [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Selvaraju_Taking_a_HINT_Leveraging_Explanations_to_Make_Vision_and_Language_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Taking%20a%20HINT%20-%20Leveraging%20Explanations%20to%20Make%20Vision%20and%20Language%20Models%20More%20Grounded.bib).
- [2021 AAAI] **Regularizing Attention Networks for Anomaly Detection in Visual Question Answering**, [[paper]](https://arxiv.org/pdf/2009.10054.pdf), [[bibtex]](/Bibtex/Regularizing%20Attention%20Networks%20for%20Anomaly%20Detection%20in%20Visual%20Question%20Answering.bib), sources: [[LeeDoYup/Anomaly_Detection_VQA]](https://github.com/LeeDoYup/Anomaly_Detection_VQA).
- [2020 CVPR] **Counterfactual Samples Synthesizing for Robust Visual Question Answering**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Counterfactual_Samples_Synthesizing_for_Robust_Visual_Question_Answering_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Counterfactual%20Samples%20Synthesizing%20for%20Robust%20Visual%20Question%20Answering.bib), sources: [[yanxinzju/CSS-VQA]](https://github.com/yanxinzju/CSS-VQA).
- [2020 CVPR] **Counterfactual Vision and Language Learning**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Abbasnejad_Counterfactual_Vision_and_Language_Learning_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Counterfactual%20Vision%20and%20Language%20Learning.bib).
- [2020 CVPR] **Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Agarwal_Towards_Causal_VQA_Revealing_and_Reducing_Spurious_Correlations_by_Invariant_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Towards%20Causal%20VQA%20-%20Revealing%20and%20Reducing%20Spurious%20Correlations%20by%20Invariant%20and%20Covariant%20Semantic%20Editing.bib), [[homepage]](https://rakshithshetty.github.io/CausalVQA/), sources: [[AgarwalVedika/CausalVQA]](https://github.com/AgarwalVedika/CausalVQA).
- [2020 ECCV] **Learning What Makes a Difference from Counterfactual Examples and Gradient Supervision**, [[paper]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550579.pdf), [[bibtex]](/Bibtex/Learning%20What%20Makes%20a%20Difference%20from%20Counterfactual%20Examples%20and%20Gradient%20Supervision.bib).
- [2020 EMNLP] **Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/2020.emnlp-main.265.pdf), [[bibtex]](https://www.aclweb.org/anthology/2020.emnlp-main.265.bib).
- [2021 CVPR] **Counterfactual VQA: A Cause-Effect Look at Language Bias**, [[paper]](https://arxiv.org/pdf/2006.04315.pdf), [[bibtex]](/Bibtex/Counterfactual%20VQA%20-%20A%20Cause-Effect%20Look%20at%20Language%20Bias.bib).
- [2021 CVPR] **Causal Attention for Vision-Language Tasks**, [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Causal_Attention_for_Vision-Language_Tasks_CVPR_2021_paper.pdf), [[bibtex]](/Bibtex/Causal%20Attention%20for%20Vision-Language%20Tasks.bib), [[supplementary]](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Yang_Causal_Attention_for_CVPR_2021_supplemental.pdf), sources: [[yangxuntu/lxmertcatt]](https://github.com/yangxuntu/lxmertcatt).

## Visual Reasoning and Referring Expression
- [2018 CVPR] **Referring Relationships**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Krishna_Referring_Relationships_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Referring%20Relationships.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/referringrelationships/), sources: [[StanfordVL/ReferringRelationships]](https://github.com/StanfordVL/ReferringRelationships).
- [2019 ACL] **A Corpus for Reasoning About Natural Language Grounded in Photographs**, [[paper]](https://www.aclweb.org/anthology/P19-1644v2.pdf), [[bibtex]](/Bibtex/A%20Corpus%20for%20Reasoning%20About%20Natural%20Language%20Grounded%20in%20Photographs.bib), [[homepage]](http://lil.nlp.cornell.edu/nlvr/).
- [2019 ICCV] **Dynamic Graph Attention for Referring Expression Comprehension**, [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Dynamic_Graph_Attention_for_Referring_Expression_Comprehension_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Dynamic%20Graph%20Attention%20for%20Referring%20Expression%20Comprehension.bib), sources: [[sibeiyang/sgmn]](https://github.com/sibeiyang/sgmn).
- [2019 CVPR] **From Recognition to Cognition: Visual Commonsense Reasoning**, [[paper]](https://arxiv.org/pdf/1811.10830.pdf), [[bibtex]](/Bibtex/From%20Recognition%20to%20Cognition%20-%20Visual%20Commonsense%20Reasoning.bib), [[homepage]](https://visualcommonsense.com), [[leaderboard]](https://visualcommonsense.com/leaderboard/), [[dataset]](https://visualcommonsense.com/download/), sources: [[rowanz/r2c]](https://github.com/rowanz/r2c/).
- [2019 NeurIPS] **TAB-VCR: Tags and Attributes based VCR Baselines**, [[paper]](https://papers.nips.cc/paper/9693-tab-vcr-tags-and-attributes-based-vcr-baselines.pdf), [[bibtex]](/Bibtex/TAB-VCR%20-%20Tags%20and%20Attributes%20based%20VCR%20Baselines.bib), [[slides]](https://deanplayerljx.github.io/tabvcr/neurips_2019_slides.pdf), [[homepage]](https://deanplayerljx.github.io/tabvcr/), sources: [[Deanplayerljx/tab-vcr]](https://github.com/Deanplayerljx/tab-vcr).
- [2020 CVPR] **Graph-Structured Referring Expression Reasoning in The Wild**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Graph-Structured_Referring_Expression_Reasoning_in_the_Wild_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Graph-Structured%20Referring%20Expression%20Reasoning%20in%20The%20Wild.bib), sources: [[sibeiyang/sgmn]](https://github.com/sibeiyang/sgmn).
- [2021 WACV] **Meta Module Network for Compositional Visual Reasoning**, [[paper]](https://arxiv.org/pdf/1910.03230.pdf), [[bibtex]](/Bibtex/Meta%20Module%20Network%20for%20Compositional%20Visual%20Reasoning.bib), sources: [[wenhuchen/Meta-Module-Network]](https://github.com/wenhuchen/Meta-Module-Network).
- [2021 AAAI] **Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding**, [[paper]](https://arxiv.org/pdf/2009.01449.pdf), [[bibtex]](/Bibtex/Ref-NMS%20-%20Breaking%20Proposal%20Bottlenecks%20in%20Two-Stage%20Referring%20Expression%20Grounding.bib), sources: [[ChopinSharp/ref-nms]](https://github.com/ChopinSharp/ref-nms).
- [2021 ArXiv] **VL-NMS: Breaking Proposal Bottlenecks in Two-Stage Visual-Language Matching**, [[paper]](https://arxiv.org/pdf/2105.05636.pdf), [[bibtex]](/Bibtex/VL-NMS%20-%20Breaking%20Proposal%20Bottlenecks%20in%20Two-Stage%20Visual-Language%20Matching.bib).


## Image-based Visual Dialogue
- [2020 CVPR] **Two Causal Principles for Improving Visual Dialog**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_Two_Causal_Principles_for_Improving_Visual_Dialog_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Two%20Causal%20Principles%20for%20Improving%20Visual%20Dialog%20-%20Visual%20Dialog%20Challenge%202019%20Winner%20Report.bib), sources: [[simpleshinobu/visdial-principles]](https://github.com/simpleshinobu/visdial-principles).