# Visual Grounding/Localization

- Resources: [[yawenzeng/Awesome-Cross-Modal-Video-Moment-Retrieval]](https://github.com/yawenzeng/Awesome-Cross-Modal-Video-Moment-Retrieval).
- Resources: [[Soldelli/Awesome-Temporal-Language-Grounding-in-Videos]](https://github.com/Soldelli/Awesome-Temporal-Language-Grounding-in-Videos).

## Survey
- [2020 ICCST] **A Survey of Temporal Activity Localization via Language in Untrimmed Videos**, [[paper]](/Documents/Papers/A%20Survey%20of%20Temporal%20Activity%20Localization%20via%20Language%20in%20Untrimmed%20Videos.pdf), [[bibtex]](/Bibtex/A%20Survey%20of%20Temporal%20Activity%20Localization%20via%20Language%20in%20Untrimmed%20Videos.bib).
- [2021 ArXiv] **A Survey on Natural Language Video Localization**, [[paper]](https://arxiv.org/pdf/2104.00234.pdf), [[bibtex]](/Bibtex/A%20Survey%20on%20Natural%20Language%20Video%20Localization.bib).
- [2021 ArXiv] **A Survey on Temporal Sentence Grounding in Videos**, [[paper]](https://arxiv.org/pdf/2109.08039.pdf), [[bibtex]](/Bibtex/A%20Survey%20on%20Temporal%20Sentence%20Grounding%20in%20Videos.bib).
- [2022 ArXiv] **The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions**, [[paper]](https://arxiv.org/pdf/2201.08071.pdf), [[bibtex]](/Bibtex/The%20Elements%20of%20Temporal%20Sentence%20Grounding%20in%20Videos%20-%20A%20Survey%20and%20Future%20Directions.bib).

## Temporal Video Grounding
- [2017 ICCV] **Localizing Moments in Video with Natural Language**, [[paper]](https://arxiv.org/pdf/1708.01641.pdf), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Natural%20Language.bib), sources: [[LisaAnne/LocalizingMoments]](https://github.com/LisaAnne/LocalizingMoments).
- [2017 ICCV] **TALL: Temporal Activity Localization via Language Query**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Gao_TALL_Temporal_Activity_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/TALL%20-%20Temporal%20Activity%20Localization%20via%20Language%20Query.bib), sources: [[jiyanggao/TALL]](https://github.com/jiyanggao/TALL).
- [2018 EMNLP] **Localizing Moments in Video with Temporal Language**, [[paper]](https://www.aclweb.org/anthology/D18-1168.pdf), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Temporal%20Language.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/D18-1168.Attachment.pdf), sources: [[LisaAnne/TemporalLanguageRelease]](https://github.com/LisaAnne/TemporalLanguageRelease).
- [2018 EMNLP] **Temporally Grounding Natural Sentence in Video**, [[paper]](https://aclanthology.org/D18-1015.pdf), [[bibtex]](/Bibtex/Temporally%20Grounding%20Natural%20Sentence%20in%20Video.bib).
- [2018 ECCV] **Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Bingbin_Liu_Temporal_Modular_Networks_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Temporal%20Modular%20Networks%20for%20Retrieving%20Complex%20Compositional%20Activities%20in%20Videos.bib), [[homepage]](https://clarabing.github.io/tmn/).
- [2018 ECCV] **Find and Focus: Retrieve and Localize Video Events with Natural Language Queries**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Dian_SHAO_Find_and_Focus_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Find%20and%20Focus%20-%20Retrieve%20and%20Localize%20Video%20Events%20with%20Natural%20Language%20Queries.bib).
- [2018 ACMMM] **Cross-modal Moment Localization in Videos**, [[paper]](https://liqiangnie.github.io/paper/p843-liu.pdf), [[bibtex]](/Bibtex/Cross-modal%20Moment%20Localization%20in%20Videos.bib).
- [2018 ArXiv] **Attentive Sequence to Sequence Translation for Localizing Clips of Interest by Natural Language Descriptions**, [[paper]](https://arxiv.org/pdf/1808.08803.pdf), [[bibtex]](/Bibtex/Attentive%20Sequence%20to%20Sequence%20Translation%20for%20Localizing%20Clips%20of%20Interest%20by%20Natural%20Language%20Descriptions.bib), sources: [[NeonKrypton/ASST]](https://github.com/NeonKrypton/ASST).
- [2018 ArXiv] **Text-to-Clip Video Retrieval with Early Fusion and Re-Captioning**, [[paper]](http://cs-people.bu.edu/hxu/arxiv-version-Text-to-Clip.pdf), [[bibtex]](/Bibtex/Text-to-Clip%20Video%20Retrieval%20with%20Early%20Fusion%20and%20Re-Captioning.bib).
- [2018 SIGIR] **Attentive Moment Retrieval in Videos**, [[paper]](http://staff.ustc.edu.cn/~hexn/papers/sigir18-video-retrieval.pdf), [[bibtex]](/Bibtex/Attentive%20Moment%20Retrieval%20in%20Videos.bib), [[slides]](https://pdfs.semanticscholar.org/5dc8/f69ad9404ed9e8d2318dca19f4eb534440a5.pdf), [[codes]](https://sigir2018.wixsite.com/acrn).
- [2019 WACV] **MAC: Mining Activity Concepts for Language-based Temporal Localization**, [[paper]](https://arxiv.org/pdf/1811.08925.pdf), [[bibtex]](/Bibtex/MAC%20-%20Mining%20Activity%20Concepts%20for%20Language-based%20Temporal%20Localization.bib), sources: [[runzhouge/MAC]](https://github.com/runzhouge/MAC).
- [2019 NAACL] **ExCL: Extractive Clip Localization Using Natural Language Descriptions**, [[paper]](https://www.aclweb.org/anthology/N19-1198.pdf), [[bibtex]](/Bibtex/ExCL%20-%20Extractive%20Clip%20Localization%20Using%20Natural%20Language%20Descriptions.bib).
- [2019 CVPR] **MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_MAN_Moment_Alignment_Network_for_Natural_Language_Moment_Retrieval_via_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/MAN%20-%20Moment%20Alignment%20Network%20for%20Natural%20Language%20Moment%20Retrieval%20via%20Iterative%20Graph%20Adjustment.bib).
- [2019 CVPR] **Language-driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Language-Driven_Temporal_Activity_Localization_A_Semantic_Matching_Reinforcement_Learning_Model_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Language-driven%20Temporal%20Activity%20Localization%20-%20A%20Semantic%20Matching%20Reinforcement%20Learning%20Model.bib).
- [2019 AAAI] **Localizing Natural Language in Videos**, [[paper]](http://forestlinma.com/welcome_files/Jingyuan_Chen_Localizing_Natural_Language_In_Videos_AAAI_2019.pdf), [[bibtex]](/Bibtex/Localizing%20Natural%20Language%20in%20Videos.bib).
- [2019 AAAI] **Multilevel Language and Vision Integration for Text-to-Clip Retrieval**, [[paper]](https://arxiv.org/pdf/1804.05113.pdf), [[bibtex]](/Bibtex/Multilevel%20Language%20and%20Vision%20Integration%20for%20Text-to-Clip%20Retrieval.bib), sources: [[VisionLearningGroup/Text-to-Clip_Retrieval]](https://github.com/VisionLearningGroup/Text-to-Clip_Retrieval).
- [2019 AAAI] **To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression**, [[paper]](https://arxiv.org/pdf/1804.07014.pdf), [[bibtex]](/Bibtex/To%20Find%20Where%20You%20Talk%20-%20Temporal%20Sentence%20Localization%20in%20Video%20with%20Attention%20Based%20Location%20Regression.bib).
- [2019 AAAI] **Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos**, [[paper]](https://arxiv.org/pdf/1901.06829.pdf), [[bibtex]](/Bibtex/Read%20Watch%20and%20Move%20-%20Reinforcement%20Learning%20for%20Temporally%20Grounding%20Natural%20Language%20Descriptions%20in%20Videos.bib).
- [2019 AAAI] **Semantic Proposal for Activity Localization in Videos via Sentence Query**, [[paper]](https://pdfs.semanticscholar.org/8548/d5a93869a5a4c808f5e81742f59f848c718c.pdf?_ga=2.88458585.398432507.1574674952-963912669.1574674952), [[bibtex]](/Bibtex/Semantic%20Proposal%20for%20Activity%20Localization%20in%20Videos%20via%20Sentence%20Query.bib).
- [2019 ACMMM] **Exploiting Temporal Relationships in Video Moment Localization with Natural Language**, [[paper]](https://arxiv.org/pdf/1908.03846.pdf), [[bibtex]](/Bibtex/Exploiting%20Temporal%20Relationships%20in%20Video%20Moment%20Localization%20with%20Natural%20Language.bib), sources: [[Sy-Zhang/TCMN-Release]](https://github.com/Sy-Zhang/TCMN-Release).
- [2019 SIGIR] **Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos**, [[paper]](https://arxiv.org/pdf/1906.02497.pdf), [[bibtex]](/Bibtex/Cross-Modal%20Interaction%20Networks%20for%20Query-Based%20Moment%20Retrieval%20in%20Videos.bib), sources: [[ikuinen/CMIN_moment_retrieval]](https://github.com/ikuinen/CMIN_moment_retrieval).
- [2019 EMNLP] **DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization**, [[paper]](https://www.aclweb.org/anthology/D19-1518.pdf), [[bibtex]](/Bibtex/DEBUG%20-%20A%20Dense%20Bottom-Up%20Grounding%20Approach%20for%20Natural%20Language%20Video%20Localization.bib).
- [2019 NeurIPS] **Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos**, [[paper]](https://papers.nips.cc/paper/8344-semantic-conditioned-dynamic-modulation-for-temporal-sentence-grounding-in-videos.pdf), [[bibtex]](/Bibtex/Semantic%20Conditioned%20Dynamic%20Modulation%20for%20Temporal%20Sentence%20Grounding%20in%20Videos.bib), sources: [[yytzsy/SCDM]](https://github.com/yytzsy/SCDM).
- [2020 BMVC] **Tripping through time: Efficient Localization of Activities in Videos**, [[paper]](https://arxiv.org/pdf/1904.09936.pdf), [[bibtex]](/Bibtex/Tripping%20through%20time%20-%20Efficient%20Localization%20of%20Activities%20in%20Videos.bib).
- [2020 WACV] **Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention**, [[paper]](http://openaccess.thecvf.com/content_WACV_2020/papers/Rodriguez_Proposal-free_Temporal_Moment_Localization_of_a_Natural-Language_Query_in_Video_WACV_2020_paper.pdf), [[bibtex]](/Bibtex/Proposal-free%20Temporal%20Moment%20Localization%20of%20a%20Natural-Language%20Query%20in%20Video%20using%20Guided%20Attention.bib), sources: [[crodriguezo/TMLGA]](https://github.com/crodriguezo/TMLGA).
- [2020 AAAI] **Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language**, [[paper]](https://arxiv.org/pdf/1912.03590.pdf), [[bibtex]](/Bibtex/Learning%202D%20Temporal%20Adjacent%20Networks%20for%20Moment%20Localization%20with%20Natural%20Language.bib), sources: [[microsoft/2D-TAN]](https://github.com/microsoft/2D-TAN), [[ChenJoya/2dtan]](https://github.com/ChenJoya/2dtan).
- [2020 AAAI] **Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video**, [[paper]](https://arxiv.org/pdf/2001.06680.pdf), [[bibtex]](/Bibtex/Tree-Structured%20Policy%20based%20Progressive%20Reinforcement%20Learning%20for%20Temporally%20Language%20Grounding%20in%20Video.bib), sources: [[WuJie1010/TSP-PRL]](https://github.com/WuJie1010/TSP-PRL).
- [2020 AAAI] **Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction**, [[paper]](https://arxiv.org/pdf/1909.05010.pdf), [[bibtex]](/Bibtex/Temporally%20Grounding%20Language%20Queries%20in%20Videos%20by%20Contextual%20Boundary-aware%20Prediction.bib), sources: [[JaywongWang/CBP]](https://github.com/JaywongWang/CBP).
- [2020 AAAI] **Rethinking the Bottom-Up Framework for Query-based Video Localization**, [[paper]](https://zjuchenlong.github.io/papers/AAAI_2020.pdf), [[bibtex]](/Bibtex/Rethinking%20the%20Bottom-Up%20Framework%20for%20Query-based%20Video%20Localization.bib).
- [2020 ACL] **Span-based Localizing Network for Natural Language Video Localization**, [[paper]](https://www.aclweb.org/anthology/2020.acl-main.585.pdf), [[bibtex]](https://www.aclweb.org/anthology/2020.acl-main.585.bib), sources: [[IsaacChanghau/VSLNet]](https://github.com/IsaacChanghau/VSLNet).
- [2020 CVPR] **Dense Regression Network for Video Grounding**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zeng_Dense_Regression_Network_for_Video_Grounding_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Dense%20Regression%20Network%20for%20Video%20Grounding.bib), [[supplementary]](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Zeng_Dense_Regression_Network_CVPR_2020_supplemental.pdf), sources: [[Alvin-Zeng/DRN]](https://github.com/Alvin-Zeng/DRN).
- [2020 CVPR] **Local-Global Video-Text Interactions for Temporal Grounding**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Mun_Local-Global_Video-Text_Interactions_for_Temporal_Grounding_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Local-Global%20Video-Text%20Interactions%20for%20Temporal%20Grounding.bib), sources: [[JonghwanMun/LGI4temporalgrounding]](https://github.com/JonghwanMun/LGI4temporalgrounding).
- [2020 ECCV] **Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos**, [[paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490324.pdf), [[bibtex]](/Bibtex/Learning%20Modality%20Interaction%20for%20Temporal%20Sentence%20Localization%20and%20Event%20Captioning%20in%20Videos.bib).
- [2020 ACMMM] **STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localization**, [[paper]](/Documents/Papers/STRONG%20-%20Spatio-Temporal%20Reinforcement%20Learning%20for%20Cross-Modal%20Video%20Moment%20Localization.pdf), [[bibtex]](/Bibtex/STRONG%20-%20Spatio-Temporal%20Reinforcement%20Learning%20for%20Cross-Modal%20Video%20Moment%20Localization.bib).
- [2020 ACMMM] **Adversarial Video Moment Retrieval by Jointly Modeling Ranking and Localization**, [[paper]](/Documents/Papers/Adversarial%20Video%20Moment%20Retrieval%20by%20Jointly%20Modeling%20Ranking%20and%20Localization.pdf), [[bibtex]](/Bibtex/Adversarial%20Video%20Moment%20Retrieval%20by%20Jointly%20Modeling%20Ranking%20and%20Localization.bib).
- [2020 ACMMM] **Dual Path Interaction Network for Video Moment Localization**, [[paper]](/Documents/Papers/Dual%20Path%20Interaction%20Network%20for%20Video%20Moment%20Localization.pdf), [[bibtex]](/Bibtex/Dual%20Path%20Interaction%20Network%20for%20Video%20Moment%20Localization.bib).
- [2020 ACMMM] **Fine-grained Iterative Attention Network for Temporal Language Localization in Videos**, [[paper]](https://arxiv.org/pdf/2008.02448.pdf), [[bibtex]](/Bibtex/Fine-grained%20Iterative%20Attention%20Network%20for%20Temporal%20Language%20Localization%20in%20Videos.bib).
- [2020 ACMMM] **Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization**, [[paper]](https://arxiv.org/pdf/2008.01403.pdf), [[bibtex]](/Bibtex/Jointly%20Cross-%20and%20Self-Modal%20Graph%20Attention%20Network%20for%20Query-Based%20Moment%20Localization.bib), sources: [[liudaizong/CSMGAN]](https://github.com/liudaizong/CSMGAN).
- [2020 COLING] **Reasoning Step-by-Step: Temporal Sentence Localization in Videos via Deep Rectification-Modulation Network**, [[paper]](https://www.aclweb.org/anthology/2020.coling-main.167.pdf), [[bibtex]](https://www.aclweb.org/anthology/2020.coling-main.167.bib).
- [2020 TPAMI] **Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos**, [[paper]](/Documents/Papers/Semantic%20Conditioned%20Dynamic%20Modulation%20for%20Temporal%20Sentence%20Grounding%20in%20Videos%20TPAMI.pdf), [[bibtex]](/Bibtex/Semantic%20Conditioned%20Dynamic%20Modulation%20for%20Temporal%20Sentence%20Grounding%20in%20Videos%20TPAMI.bib), sources: [[yytzsy/SCDM]](https://github.com/yytzsy/SCDM).
- [2020 ArXiv] **A Simple Yet Effective Method for Video Temporal Grounding with Cross-Modality Attention**, [[paper]](https://arxiv.org/pdf/2009.11232.pdf), [[bibtex]](/Bibtex/A%20Simple%20Yet%20Effective%20Method%20for%20Video%20Temporal%20Grounding%20with%20Cross-Modality%20Attention.bib).
- [2020 ArXiv] **Boundary-sensitive Pre-training for Temporal Localization in Videos**, [[paper]](https://arxiv.org/pdf/2011.10830.pdf), [[bibtex]](/Bibtex/Boundary-sensitive%20Pre-training%20for%20Temporal%20Localization%20in%20Videos.bib).
- [2020 ArXiv] **Multi-Scale 2D Temporal Adjacent Networks for Moment Localization with Natural Language**, [[paper]](https://arxiv.org/pdf/2012.02646.pdf), [[bibtex]](/Bibtex/Multi-Scale%202D%20Temporal%20Adjacent%20Networks%20for%20Moment%20Localization%20with%20Natural%20Language.bib), sources: [[microsoft/2D-TAN]](https://github.com/microsoft/2D-TAN), [[ChenJoya/2dtan]](https://github.com/ChenJoya/2dtan).
- [2020 ArXiv] **VLG-Net: Video-Language Graph Matching Network for Video Grounding**, [[paper]](https://arxiv.org/pdf/2011.10132.pdf), [[bibtex]](/Bibtex/VLG-Net%20-%20Video-Language%20Graph%20Matching%20Network%20for%20Video%20Grounding.bib).
- [2020 JNCA] **Context-aware Network with Foreground Recalibration for Grounding Natural Language in Video**, [[paper]](/Documents/Papers/Context-aware%20Network%20with%20Foreground%20Recalibration%20for%20Grounding%20Natural%20Language%20in%20Video.pdf), [[bibtex]](/Bibtex/Context-aware%20Network%20with%20Foreground%20Recalibration%20for%20Grounding%20Natural%20Language%20in%20Video.bib).
- [2021 WACV] **DORi: Discovering Object Relationship for Moment Localization of a Natural-Language Query in Video**, [[paper]](https://openaccess.thecvf.com/content/WACV2021/papers/Rodriguez-Opazo_DORi_Discovering_Object_Relationships_for_Moment_Localization_of_a_Natural_WACV_2021_paper.pdf), [[bibtex]](/Bibtex/DORi%20-%20Discovering%20Object%20Relationship%20for%20Moment%20Localization%20of%20a%20Natural-Language%20Query%20in%20Video.bib), sources: [[crodriguezo/dori]](https://github.com/crodriguezo/dori).
- [2021 AAAI] **Boundary Proposal Network for Two-Stage Natural Language Video Localization**, [[paper]](https://arxiv.org/pdf/2103.08109.pdf), [[bibtex]](/Bibtex/Boundary%20Proposal%20Network%20for%20Two-Stage%20Natural%20Language%20Video%20Localization.bib).
- [2021 AAAI] **Proposal-Free Video Grounding with Contextual Pyramid Network**, [[paper]](https://www.aaai.org/AAAI21Papers/AAAI-9134.LiK.pdf), [[bibtex]](/Bibtex/Proposal-Free%20Video%20Grounding%20with%20Contextual%20Pyramid%20Network.bib).
- [2021 TMM] **Frame-wise Cross-modal Match for Video Moment Retrieval**, [[paper]](https://arxiv.org/pdf/2009.10434.pdf), [[bibtex]](/Bibtex/Frame-wise%20Cross-modal%20Match%20for%20Video%20Moment%20Retrieval.bib), sources: [[tanghaoyu258/ACRM-for-moment-retrieval]](https://github.com/tanghaoyu258/ACRM-for-moment-retrieval).
- [2021 TIP] **Interaction-Integrated Network for Natural Language Moment Localization**, [[paper]](/Documents/Papers/Interaction-Integrated%20Network%20for%20Natural%20Language%20Moment%20Localization.pdf), [[bibtex]](/Bibtex/Interaction-Integrated%20Network%20for%20Natural%20Language%20Moment%20Localization.bib).
- [2021 TPAMI] **Natural Language Video Localization: A Revisit in Span-based Question Answering Framework**, [[paper]](https://arxiv.org/pdf/2102.13558.pdf), [[bibtex]](/Bibtex/Natural%20Language%20Video%20Localization%20-%20A%20Revisit%20in%20Span-based%20Question%20Answering%20Framework.bib).
- [2021 CVPR] **Context-aware Biaffine Localizing Network for Temporal Sentence Grounding**, [[paper]](https://arxiv.org/pdf/2103.11555.pdf), [[bibtex]](/Bibtex/Context-aware%20Biaffine%20Localizing%20Network%20for%20Temporal%20Sentence%20Grounding.bib), sources: [[liudaizong/CBLN]](https://github.com/liudaizong/CBLN).
- [2021 CVPR] **Interventional Video Grounding with Dual Contrastive Learning**, [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Nan_Interventional_Video_Grounding_With_Dual_Contrastive_Learning_CVPR_2021_paper.pdf), [[bibtex]](/Bibtex/Interventional%20Video%20Grounding%20with%20Dual%20Contrastive%20Learning.bib), [[supplementary]](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Nan_Interventional_Video_Grounding_CVPR_2021_supplemental.pdf), sources: [[nanguoshun/IVG]](https://github.com/nanguoshun/IVG).
- [2021 CVPR] **Cascaded Prediction Network via Segment Tree for Temporal Video Grounding**, [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Cascaded_Prediction_Network_via_Segment_Tree_for_Temporal_Video_Grounding_CVPR_2021_paper.pdf), [[bibtex]](/Bibtex/Cascaded%20Prediction%20Network%20via%20Segment%20Tree%20for%20Temporal%20Video%20Grounding.bib), [[supplementary]](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Zhao_Cascaded_Prediction_Network_CVPR_2021_supplemental.pdf).
- [2021 CVPR] **Multi-stage Aggregated Transformer Network for Temporal Language Localization in Videos**, [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Multi-Stage_Aggregated_Transformer_Network_for_Temporal_Language_Localization_in_Videos_CVPR_2021_paper.pdf), [[bibtex]](/Bibtex/Multi-stage%20Aggregated%20Transformer%20Network%20for%20Temporal%20Language%20Localization%20in%20Videos.bib).
- [2021 ACL] **Parallel Attention Network with Sequence Matching for Video Grounding**, [[paper]](https://aclanthology.org/2021.findings-acl.69.pdf), [[bibtex]](/Bibtex/Parallel%20Attention%20Network%20with%20Sequence%20Matching%20for%20Video%20Grounding.bib), sources: [[IsaacChanghau/SeqPAN]](https://github.com/IsaacChanghau/SeqPAN).
- [2021 EMNLP] **Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos**, [[paper]](https://arxiv.org/pdf/2109.06398.pdf), [[bibtex]](/Bibtex/Adaptive%20Proposal%20Generation%20Network%20for%20Temporal%20Sentence%20Localization%20in%20Videos.bib).
- [2021 EMNLP] **Natural Language Video Localization with Learnable Moment Proposals**, [[paper]](https://arxiv.org/pdf/2109.10678.pdf), [[bibtex]](/Bibtex/Natural%20Language%20Video%20Localization%20with%20Learnable%20Moment%20Proposals.bib).
- [2021 EMNLP] **Relation-aware Video Reading Comprehension for Temporal Language Grounding**, [[paper]](https://aclanthology.org/2021.emnlp-main.324.pdf), [[bibtex]](/Bibtex/Relation-aware%20Video%20Reading%20Comprehension%20for%20Temporal%20Language%20Grounding.bib), sources: [[Huntersxsx/RaNet]](https://github.com/Huntersxsx/RaNet).
- [2021 ICME] **Diving Into The Relations: Leveraging Semantic and Visual Structures For Video Moment Retrieval**, [[paper]](/Documents/Papers/Diving%20Into%20The%20Relations%20-%20Leveraging%20Semantic%20and%20Visual%20Structures%20For%20Video%20Moment%20Retrieval.pdf), [[bibtex]](/Bibtex/Diving%20Into%20The%20Relations%20-%20Leveraging%20Semantic%20and%20Visual%20Structures%20For%20Video%20Moment%20Retrieval.bib).
- [2021 ICCV] **Fast Video Moment Retrieval**, [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Fast_Video_Moment_Retrieval_ICCV_2021_paper.pdf), [[bibtex]](/Bibtex/Fast%20Video%20Moment%20Retrieval.bib).
- [2021 ArXiv] **Progressive Localization Networks for Language-based Moment Localization**, [[paper]](https://arxiv.org/pdf/2102.01282.pdf), [[bibtex]](/Bibtex/Progressive%20Localization%20Networks%20for%20Language-based%20Moment%20Localization.bib).
- [2021 ArXiv] **Decoupled Spatial Temporal Graphs for Generic Visual Grounding**, [[paper]](https://arxiv.org/pdf/2103.10191.pdf), [[bibtex]](/Bibtex/Decoupled%20Spatial%20Temporal%20Graphs%20for%20Generic%20Visual%20Grounding.bib).
- [2021 ArXiv] **VGNMN: Video-grounded Neural Module Network to Video-Grounded Language Tasks**, [[paper]](https://arxiv.org/pdf/2104.07921.pdf), [[bibtex]](/Bibtex/VGNMN%20-%20Video-grounded%20Neural%20Module%20Network%20to%20Video-Grounded%20Language%20Tasks.bib).
- [2021 ArXiv] **End-to-end Multi-modal Video Temporal Grounding**, [[paper]](https://arxiv.org/pdf/2107.05624.pdf), [[bibtex]](/Bibtex/End-to-end%20Multi-modal%20Video%20Temporal%20Grounding.bib).
- [2021 ArXiv] **QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries**, [[paper]](https://arxiv.org/pdf/2107.09609.pdf), [[bibtex]](/Bibtex/QVHighlights%20-%20Detecting%20Moments%20and%20Highlights%20in%20Videos%20via%20Natural%20Language%20Queries.bib), sources: [[jayleicn/moment_detr]](https://github.com/jayleicn/moment_detr).
- [2021 ArXiv] **Hierarchical Deep Residual Reasoning for Temporal Moment Localization**, [[paper]](https://arxiv.org/pdf/2111.00417.pdf), [[bibtex]](/Bibtex/Hierarchical%20Deep%20Residual%20Reasoning%20for%20Temporal%20Moment%20Localization.bib).

## Weakly/Self/Semi/Un- Supervised Temporal Video Grounding
- [2015 ICCV] **Weakly-Supervised Alignment of Video With Text**, [[paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Bojanowski_Weakly-Supervised_Alignment_of_ICCV_2015_paper.pdf), [[bibtex]](/Bibtex/Weakly-Supervised%20Alignment%20of%20Video%20With%20Text.bib).
- [2018 NeurIPS] **Weakly Supervised Dense Event Captioning in Videos**, [[paper]](https://papers.nips.cc/paper/7569-weakly-supervised-dense-event-captioning-in-videos.pdf), [[bibtex]](/Bibtex/Weakly%20Supervised%20Dense%20Event%20Captioning%20in%20Videos.bib), sources: [[XgDuan/WSDEC]](https://github.com/XgDuan/WSDEC).
- [2019 CVPR] **Weakly Supervised Video Moment Retrieval From Text Queries**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Mithun_Weakly_Supervised_Video_Moment_Retrieval_From_Text_Queries_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Weakly%20Supervised%20Video%20Moment%20Retrieval%20From%20Text%20Queries.bib), sources: [[niluthpol/weak_supervised_video_moment]](https://github.com/niluthpol/weak_supervised_video_moment).
- [2019 EMNLP] **WSLLN: Weakly Supervised Natural Language Localization Networks**, [[paper]](https://www.aclweb.org/anthology/D19-1157.pdf), [[bibtex]](https://www.aclweb.org/anthology/D19-1157.bib).
- [2020 AAAI] **Weakly-Supervised Video Moment Retrieval via Semantic Completion Network**, [[paper]](https://arxiv.org/pdf/1911.08199.pdf), [[bibtex]](/Bibtex/Weakly-Supervised%20Video%20Moment%20Retrieval%20via%20Semantic%20Completion%20Network.bib).
- [2020 ArXiv] **Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video**, [[paper]](https://arxiv.org/pdf/2001.09308.pdf), [[bibtex]](/Bibtex/Look%20Closer%20to%20Ground%20Better%20-%20Weakly-Supervised%20Temporal%20Grounding%20of%20Sentence%20in%20Video.bib).
- [2020 ArXiv] **Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos**, [[paper]](https://arxiv.org/pdf/2003.07048.pdf), [[bibtex]](/Bibtex/Weakly-Supervised%20Multi-Level%20Attentional%20Reconstruction%20Network%20for%20Grounding%20Textual%20Queries%20in%20Videos.bib).
- [2020 ACMMM] **Regularized Two-Branch Proposal Networks for Weakly-Supervised Moment Retrieval in Videos**, [[paper]](https://arxiv.org/pdf/2008.08257.pdf), [[bibtex]](/Bibtex/Regularized%20Two-Branch%20Proposal%20Networks%20for%20Weakly-Supervised%20Moment%20Retrieval%20in%20Videos.bib), sources: [[ikuinen/regularized_two-branch_proposal_network]](https://github.com/ikuinen/regularized_two-branch_proposal_network).
- [2020 ACMMM] **Reinforcement Learning for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed Videos**, [[paper]](https://arxiv.org/pdf/2009.08614.pdf), [[bibtex]](/Bibtex/Reinforcement%20Learning%20for%20Weakly%20Supervised%20Temporal%20Grounding%20of%20Natural%20Language%20in%20Untrimmed%20Videos.bib).
- [2021 WACV] **LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval**, [[paper]](https://openaccess.thecvf.com/content/WACV2021/papers/Tan_LoGAN_Latent_Graph_Co-Attention_Network_for_Weakly-Supervised_Video_Moment_Retrieval_WACV_2021_paper.pdf), [[bibtex]](/Bibtex/LoGAN.bib).
- [2021 ACMMM] **AsyNCE: Disentangling False-Positives for Weakly-Supervised Video Grounding**, [[paper]](/Documents/Papers/AsyNCE%20-%20Disentangling%20False-Positives%20for%20Weakly-Supervised%20Video%20Grounding.pdf), [[bibtex]](/Bibtex/AsyNCE%20-%20Disentangling%20False-Positives%20for%20Weakly-Supervised%20Video%20Grounding.bib).
- [2021 ACMMM] **Towards Bridging Video and Language by Caption Generation and Sentence Localization**, [[paper]](/Documents/Papers/Towards%20Bridging%20Video%20and%20Language%20by%20Caption%20Generation%20and%20Sentence%20Localization.pdf), [[bibtex]](/Bibtex/Towards%20Bridging%20Video%20and%20Language%20by%20Caption%20Generation%20and%20Sentence%20Localization.bib).
- [2021 CVPR] **Towards Bridging Event Captioner and Sentence Localizer for Weakly Supervised Dense Event Captioning**, [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Towards_Bridging_Event_Captioner_and_Sentence_Localizer_for_Weakly_Supervised_CVPR_2021_paper.pdf), [[bibtex]](/Bibtex/Towards%20Bridging%20Event%20Captioner%20and%20Sentence%20Localizer%20for%20Weakly%20Supervised%20Dense%20Event%20Captioning.bib), [[supplementary]](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Chen_Towards_Bridging_Event_CVPR_2021_supplemental.pdf).
- [2021 ICCV] **Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation**, [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Cross-Sentence_Temporal_and_Semantic_Relations_in_Video_Activity_Localisation_ICCV_2021_paper.pdf), [[bibtex]](/Bibtex/Cross-Sentence%20Temporal%20and%20Semantic%20Relations%20in%20Video%20Activity%20Localisation.bib).
- [2021 ArXiv] **Self-supervised Learning for Semi-supervised Temporal Language Grounding**, [[paper]](https://arxiv.org/pdf/2109.11475.pdf), [[bibtex]](/Bibtex/Self-supervised%20Learning%20for%20Semi-supervised%20Temporal%20Language%20Grounding.bib).


## Bias in Temporal Video Grounding
- [2020 BMVC] **Uncovering Hidden Challenges in Query-Based Video Moment Retrieval**, [[paper]](https://www.bmvc2020-conference.com/assets/papers/0306.pdf), [[bibtex]](/Bibtex/Uncovering%20Hidden%20Challenges%20in%20Query-Based%20Video%20Moment%20Retrieval.bib), [[homepage]](https://www.bmvc2020-conference.com/conference/papers/paper_0306.html), sources: [[mayu-ot/hidden-challenges-MR]](https://github.com/mayu-ot/hidden-challenges-MR).
- [2021 ArXiv] **A Closer Look at Temporal Sentence Grounding in Videos: Datasets and Metrics**, [[paper]](https://arxiv.org/pdf/2101.09028.pdf), [[bibtex]](/Bibtex/A%20Closer%20Look%20at%20Temporal%20Sentence%20Grounding%20in%20Videos%20-%20Datasets%20and%20Metrics.bib), sources: [[yytzsy/grounding_changing_distribution]](https://github.com/yytzsy/grounding_changing_distribution).
- [2021 CVPR] **Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding**, [[paper]](https://arxiv.org/pdf/2103.16848.pdf), [[bibtex]](/Bibtex/Embracing%20Uncertainty%20-%20Decoupling%20and%20De-bias%20for%20Robust%20Temporal%20Grounding.bib).
- [2021 SIGIR] **Deconfounded Video Moment Retrieval with Causal Intervention**, [[paper]](https://dl.acm.org/doi/pdf/10.1145/3404835.3462823), [[bibtex]](/Bibtex/Deconfounded%20Video%20Moment%20Retrieval%20with%20Causal%20Intervention.bib), sources: [[Xun-Yang/Causal_Video_Moment_Retrieval]](https://github.com/Xun-Yang/Causal_Video_Moment_Retrieval).

## Spatio-Temporal Video Grounding
- [2019 ACL] **Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video**, [[paper]](https://www.aclweb.org/anthology/P19-1183.pdf), [[bibtex]](/Bibtex/Weakly-Supervised%20Spatio-Temporally%20Grounding%20Natural%20Sentence%20in%20Video.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/P19-1183.Supplementary.pdf), sources: [[JeffCHEN2017/WSSTG]](https://github.com/JeffCHEN2017/WSSTG).
- [2020 CVPR] **Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Where_Does_It_Exist_Spatio-Temporal_Video_Grounding_for_Multi-Form_Sentences_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Where%20Does%20It%20Exist.bib), sources: [[Guaranteer/VidSTG-Dataset]](https://github.com/Guaranteer/VidSTG-Dataset).
- [2020 ArXiv] **Human-centric Spatio-Temporal Video Grounding With Visual Transformers**, [[paper]](https://arxiv.org/pdf/2011.05049.pdf), [[bibtex]](/Bibtex/Human-centric%20Spatio-Temporal%20Video%20Grounding%20With%20Visual%20Transformers.bib), sources: [[tzhhhh123/HC-STVG]](https://github.com/tzhhhh123/HC-STVG).
- [2020 ACMMM] **Activity-driven Weakly-Supervised Spatio-Temporal Grounding from Untrimmed Videos**, [[paper]](/Documents/Papers/Activity-driven%20Weakly-Supervised%20Spatio-Temporal%20Grounding%20from%20Untrimmed%20Videos.pdf), [[bibtex]](/Bibtex/Activity-driven%20Weakly-Supervised%20Spatio-Temporal%20Grounding%20from%20Untrimmed%20Videos.bib).
- [2021 ICCV] **STVGBert: A Visual-linguistic Transformer based Framework for Spatio-temporal Video Grounding**, [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Su_STVGBert_A_Visual-Linguistic_Transformer_Based_Framework_for_Spatio-Temporal_Video_Grounding_ICCV_2021_paper.pdf), [[bibtex]](/Bibtex/STVGBert%20-%20A%20Visual-linguistic%20Transformer%20based%20Framework%20for%20Spatio-temporal%20Video%20Grounding.bib).

## Video Corpus Moment Retrieval (Video Retrieval + Moment Localization)
- [2019 ICCV] **Temporal Localization of Moments in Video Collections with Natural Language**, [[paper]](https://arxiv.org/pdf/1907.12763.pdf), [[bibtex]](/Bibtex/Temporal%20Localization%20of%20Moments%20in%20Video%20Collections%20with%20Natural%20Language.bib), sources: [[escorciav/moments-retrieval-page]](https://github.com/escorciav/moments-retrieval-page).
- [2020 ECCV] **TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval**, [[paper]](https://arxiv.org/pdf/2001.09099.pdf), [[bibtex]](/Bibtex/TVR%20-%20A%20Large-Scale%20Dataset%20for%20Video-Subtitle%20Moment%20Retrieval.bib), [[homepage]](https://tvr.cs.unc.edu), sources: [[jayleicn/TVRetrieval]](https://github.com/jayleicn/TVRetrieval).
- [2020 EMNLP] **HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training**, [[paper]](https://www.aclweb.org/anthology/2020.emnlp-main.161.pdf), [[bibtex]](https://www.aclweb.org/anthology/2020.emnlp-main.161.bib), sources: [[linjieli222/HERO]](https://github.com/linjieli222/HERO).
- [2020 ArXiv] **A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus**, [[paper]](https://arxiv.org/pdf/2011.09046.pdf), [[bibtex]](/Bibtex/A%20Hierarchical%20Multi-Modal%20Encoder%20for%20Moment%20Localization%20in%20Video%20Corpus.bib).
- [2021 SIGIR] **Video Corpus Moment Retrieval with Contrastive Learning**, [[paper]](/Documents/Papers/Video%20Corpus%20Moment%20Retrieval%20with%20Contrastive%20Learning.pdf), [[bibtex]](/Bibtex/Video%20Corpus%20Moment%20Retrieval%20with%20Contrastive%20Learning.bib), sources: [[IsaacChanghau/ReLoCLNet]](https://github.com/IsaacChanghau/ReLoCLNet).
- [2021 ACL] **mTVR: Multilingual Moment Retrieval in Videos**, [[paper]](https://aclanthology.org/2021.acl-short.92.pdf), [[bibtex]](/Bibtex/MTVR%20-%20Multilingual%20Moment%20Retrieval%20in%20Videos.bib), sources: [[jayleicn/mTVRetrieval]](https://github.com/jayleicn/mTVRetrieval).

## Other Video Groundings
### Video Re-localization
- [2018 ECCV] **Video Re-localization**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Feng_Video_Re-localization_via_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Video%20Re-localization.bib), sources: [[fengyang0317/video_reloc]](https://github.com/fengyang0317/video_reloc).

### Audio based Temporal Video Grounding
- [2018 ECCV] **Audio-Visual Event Localization in Unconstrained Videos**, [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Audio-Visual%20Event%20Localization%20in%20Unconstrained%20Videos.bib), sources: [[YapengTian/AVE-ECCV18]](https://github.com/YapengTian/AVE-ECCV18).
- [2019 ICCV] **Dual Attention Matching for Audio-Visual Event Localization**, [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Dual_Attention_Matching_for_Audio-Visual_Event_Localization_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Dual%20Attention%20Matching%20for%20Audio-Visual%20Event%20Localization.bib).

### Image based Temporal Video Grounding
- [2019 IJCAI] **Localizing Unseen Activities in Video via Image Query**, [[paper]](https://www.ijcai.org/Proceedings/2019/0610.pdf), [[bibtex]](/Bibtex/Localizing%20Unseen%20Activities%20in%20Video%20via%20Image%20Query.bib).

### Sign Language Localization
- [2021 CVPR] **Read and Attend: Temporal Localisation in Sign Language Videos**, [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Varol_Read_and_Attend_Temporal_Localisation_in_Sign_Language_Videos_CVPR_2021_paper.pdf), [[bibtex]](/Bibtex/Read%20and%20Attend%20-%20Temporal%20Localisation%20in%20Sign%20Language%20Videos.bib), [[supplementary]](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Varol_Read_and_Attend_CVPR_2021_supplemental.pdf), [[homepage]](https://www.robots.ox.ac.uk/~vgg/research/bslattend/).
